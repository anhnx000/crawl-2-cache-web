#!/bin/bash
# ALL-IN-ONE: Tá»± Ä‘á»™ng cháº¡y cáº£ proxy vÃ  crawler

cd /home/xuananh/work_1/anhnx/crawl-2

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘       ğŸš€ ALL-IN-ONE: Proxy + Crawler + Monitor            â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

# HÃ m cleanup khi Ctrl+C
cleanup() {
    echo ""
    echo "ğŸ›‘ Stopping all processes..."
    pkill -P $$ 2>/dev/null
    pkill -f "python3 app.py" 2>/dev/null
    pkill -f "auto_crawl_proxy.py" 2>/dev/null
    echo "âœ… All stopped"
    exit 0
}

trap cleanup SIGINT SIGTERM

# BÆ°á»›c 1: Kiá»ƒm tra vÃ  dá»«ng process cÅ©
echo "ğŸ“‹ Step 1: Kiá»ƒm tra processes cÅ©..."
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

if lsof -i :5002 > /dev/null 2>&1; then
    echo "   âš ï¸  Port 5002 Ä‘ang Ä‘Æ°á»£c sá»­ dá»¥ng, dá»«ng process cÅ©..."
    pkill -f "python3 app.py"
    sleep 2
fi

if pgrep -f "auto_crawl_proxy.py.*important_links" > /dev/null; then
    echo "   âš ï¸  Crawler cÅ© Ä‘ang cháº¡y, dá»«ng..."
    pkill -f "auto_crawl_proxy.py.*important_links"
    sleep 2
fi

echo "   âœ… Ready to start"
echo ""

# BÆ°á»›c 2: Start proxy
echo "ğŸ“‹ Step 2: Starting Proxy (Online Mode)..."
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

export LIVE_FALLBACK=true
export ORIGIN="https://kiagds.ru"
export LOCAL_BASE="http://localhost:5002"
export CACHE_DIR="cache"

# Cháº¡y proxy trong background
nohup python3 app.py > proxy.log 2>&1 &
PROXY_PID=$!

echo "   ğŸŒ Starting proxy (PID: $PROXY_PID)..."
sleep 3

# Kiá»ƒm tra proxy
if curl -s -o /dev/null -w "%{http_code}" "http://localhost:5002/_cache_stats" | grep -q "200"; then
    echo "   âœ… Proxy Ä‘Ã£ cháº¡y táº¡i http://localhost:5002"
    
    # Láº¥y stats
    STATS=$(curl -s "http://localhost:5002/_cache_stats" 2>/dev/null)
    CACHED=$(echo "$STATS" | python3 -c "import sys, json; print(json.load(sys.stdin).get('cached_responses', 'N/A'))" 2>/dev/null || echo "N/A")
    echo "   â€¢ Cached responses: $CACHED"
    echo ""
else
    echo "   âŒ Proxy khÃ´ng khá»Ÿi Ä‘á»™ng Ä‘Æ°á»£c!"
    echo "   ğŸ“ Xem log: cat proxy.log"
    exit 1
fi

# BÆ°á»›c 3: Start crawler
echo "ğŸ“‹ Step 3: Starting Crawler..."
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "   Total URLs: 16,304"
echo "   Follow depth: 3"
echo "   Concurrency: 5"
echo ""

# Cháº¡y crawler trong background
nohup python3 auto_crawl_proxy.py \
  --json-file important_links.json \
  --follow-depth 3 \
  --concurrency 5 \
  --delay 0.3 \
  --max-retries 10 \
  --auto-pagination \
  > cache_important_full.log 2>&1 &

CRAWLER_PID=$!
echo "   ğŸš€ Starting crawler (PID: $CRAWLER_PID)..."
sleep 3

# Kiá»ƒm tra crawler
if ps -p $CRAWLER_PID > /dev/null; then
    echo "   âœ… Crawler Ä‘Ã£ cháº¡y!"
    echo ""
else
    echo "   âŒ Crawler khÃ´ng khá»Ÿi Ä‘á»™ng Ä‘Æ°á»£c!"
    echo "   ğŸ“ Xem log: cat cache_important_full.log"
    exit 1
fi

# BÆ°á»›c 4: Monitor
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘              âœ… ÄÃƒ CHáº Y THÃ€NH CÃ”NG!                       â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ğŸ“Š Processes:"
echo "   â€¢ Proxy PID: $PROXY_PID (http://localhost:5002)"
echo "   â€¢ Crawler PID: $CRAWLER_PID"
echo ""
echo "ğŸ“ Logs:"
echo "   â€¢ Proxy: tail -f proxy.log"
echo "   â€¢ Crawler: tail -f cache_important_full.log"
echo ""
echo "ğŸ“Š Monitor:"
echo "   â€¢ Quick check: ./check_progress.sh"
echo "   â€¢ Watch: watch -n 10 './check_progress.sh'"
echo ""
echo "ğŸ›‘ Stop ALL:"
echo "   â€¢ Ctrl+C (trong terminal nÃ y)"
echo "   â€¢ pkill -f 'python3 app.py'"
echo "   â€¢ pkill -f 'auto_crawl_proxy.py'"
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "ğŸ¯ Showing crawler log (realtime)..."
echo "   Press Ctrl+C to stop monitoring (processes continue)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""

# Wait má»™t chÃºt Ä‘á»ƒ crawler báº¯t Ä‘áº§u
sleep 3

# Tail log realtime
tail -f cache_important_full.log

