#!/usr/bin/env python3
"""
Extract t·∫•t c·∫£ links c√≥ docId v√† page t·ª´ m·ªôt URL b·∫•t k·ª≥
v√† append v√†o important_links.json, ƒë·ªìng th·ªùi t·ª± ƒë·ªông cache c√°c URLs t√¨m ƒë∆∞·ª£c
"""

import asyncio
import os
import json
import sys
import re
import argparse
from urllib.parse import urlparse, urljoin, urlencode, parse_qs
import httpx
from bs4 import BeautifulSoup

# ================== CONFIG ==================
PROXY_BASE = os.getenv("LOCAL_BASE", "http://localhost:5002")
ORIGIN = os.getenv("ORIGIN", "https://kiagds.ru")
CACHE_DIR = os.getenv("CACHE_DIR", "cache")
IMPORTANT_LINKS_FILE = "important_links.json"
UA = "ExtractDocIdLinks/1.0 (+respectful; via-proxy)"
os.makedirs(CACHE_DIR, exist_ok=True)
# ============================================

def normalize_url(url: str) -> str:
    """Chu·∫©n h√≥a URL: lo·∫°i b·ªè fragment, chu·∫©n h√≥a v√† x·ª≠ l√Ω parameters r·ªóng"""
    try:
        parsed = urlparse(url)
        base_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        
        if parsed.query:
            params = parse_qs(parsed.query, keep_blank_values=False)
            cleaned_params = {}
            for key, values in params.items():
                non_empty_values = [v for v in values if v and v.strip()]
                if non_empty_values:
                    cleaned_params[key] = non_empty_values[0] if len(non_empty_values) == 1 else non_empty_values
            
            if 'page' in cleaned_params:
                page_val = cleaned_params['page']
                if isinstance(page_val, str) and (not page_val or page_val.strip() == ''):
                    del cleaned_params['page']
                elif isinstance(page_val, list) and all(not v or v.strip() == '' for v in page_val):
                    del cleaned_params['page']
            
            if cleaned_params:
                normalized = f"{base_url}?{urlencode(cleaned_params, doseq=True)}"
            else:
                normalized = base_url
        else:
            normalized = base_url
        
        return normalized
    except Exception:
        url_no_fragment = url.split('#')[0]
        url_cleaned = url_no_fragment.rstrip('=&?')
        if url_cleaned.endswith('&page') or url_cleaned.endswith('?page'):
            url_cleaned = url_cleaned[:-5]
        return url_cleaned.rstrip('&?')

def in_domain(u: str) -> bool:
    """Ki·ªÉm tra URL c√≥ thu·ªôc domain kiagds.ru kh√¥ng"""
    try:
        parsed = urlparse(u)
        netloc = parsed.netloc.lower().split(":")[0]
        return netloc.endswith("kiagds.ru")
    except Exception:
        return False

def has_docid_and_page(url: str) -> bool:
    """Ki·ªÉm tra URL c√≥ ch·ª©a docId v√† page kh√¥ng"""
    try:
        parsed = urlparse(url)
        query = parsed.query
        return 'docId=' in query and 'page=' in query
    except Exception:
        return False

def extract_docid_page_links(base_url: str, html: str) -> set:
    """
    Extract t·∫•t c·∫£ links c√≥ docId v√† page t·ª´ HTML
    """
    soup = BeautifulSoup(html, "html.parser")
    urls = set()
    
    # Extract t·ª´ c√°c th·∫ª HTML (a, link, etc.)
    for tag, attr in (
        ("a", "href"), 
        ("link", "href"), 
        ("form", "action")
    ):
        for el in soup.find_all(tag):
            href = el.get(attr)
            if not href:
                continue
            try:
                u = urljoin(base_url, href)
                if in_domain(u):
                    u = normalize_url(u)
                    if has_docid_and_page(u):
                        urls.add(u)
            except Exception:
                continue
    
    # Extract t·ª´ JavaScript (t√¨m c√°c URL trong JS)
    for script in soup.find_all("script"):
        if script.string:
            # T√¨m c√°c URL kiagds.ru c√≥ docId v√† page
            js_urls = re.findall(r'https?://kiagds\.ru[^\s"\'<>)]+', script.string)
            for u in js_urls:
                try:
                    u = normalize_url(u)
                    if in_domain(u) and has_docid_and_page(u):
                        urls.add(u)
                except Exception:
                    continue
    
    # Extract t·ª´ c√°c thu·ªôc t√≠nh data-* v√† onclick handlers
    try:
        for el in soup.find_all():
            try:
                if not hasattr(el, 'attrs') or not el.attrs:
                    continue
                attrs = el.attrs
                if not isinstance(attrs, dict):
                    continue
                
                for attr_name, attr_value in attrs.items():
                    if not isinstance(attr_value, str):
                        continue
                    
                    # Extract t·ª´ data-* attributes
                    if attr_name.startswith('data-') and 'kiagds.ru' in attr_value:
                        js_urls = re.findall(r'https?://kiagds\.ru[^\s"\'<>)]+', attr_value)
                        for u in js_urls:
                            try:
                                u = normalize_url(u)
                                if in_domain(u) and has_docid_and_page(u):
                                    urls.add(u)
                            except Exception:
                                continue
                    
                    # Extract t·ª´ onclick handlers
                    if attr_name == 'onclick' and ('docId=' in attr_value or 'kiagds.ru' in attr_value):
                        # T√¨m ajaxHref('...') ho·∫∑c c√°c h√†m t∆∞∆°ng t·ª±
                        onclick_urls = re.findall(r"(?:ajaxHref|location\.href|window\.location)\s*[=\(]\s*['\"]([^'\"]+)['\"]", attr_value)
                        for match in onclick_urls:
                            try:
                                if match.startswith('?'):
                                    u = urljoin(base_url, match)
                                elif match.startswith('/'):
                                    u = urljoin(base_url, match)
                                elif 'kiagds.ru' in match:
                                    u = match
                                else:
                                    u = urljoin(base_url, match)
                                
                                u = normalize_url(u)
                                if in_domain(u) and has_docid_and_page(u):
                                    urls.add(u)
                            except Exception:
                                continue
                        
                        # T√¨m URL pattern tr·ª±c ti·∫øp trong onclick
                        direct_urls = re.findall(r'https?://kiagds\.ru[^\s"\'<>)]+', attr_value)
                        for u in direct_urls:
                            try:
                                u = normalize_url(u)
                                if in_domain(u) and has_docid_and_page(u):
                                    urls.add(u)
                            except Exception:
                                continue
                    
                    # Extract t·ª´ c√°c attribute kh√°c c√≥ ch·ª©a docId v√† page
                    if 'docId=' in attr_value and 'page=' in attr_value:
                        # T√¨m c√°c query string pattern v·ªõi docId v√† page
                        query_urls = re.findall(r'\?mode=[^\s"\'<>)]+docId=\d+[^\s"\'<>)]*page=\d+', attr_value)
                        for qs in query_urls:
                            try:
                                u = urljoin(base_url, qs)
                                u = normalize_url(u)
                                if in_domain(u) and has_docid_and_page(u):
                                    urls.add(u)
                            except Exception:
                                continue
                        
                        # T√¨m c√°c URL ƒë·∫ßy ƒë·ªß
                        full_urls = re.findall(r'https?://kiagds\.ru[^\s"\'<>)]+', attr_value)
                        for u in full_urls:
                            try:
                                u = normalize_url(u)
                                if in_domain(u) and has_docid_and_page(u):
                                    urls.add(u)
                            except Exception:
                                continue
                                
            except (AttributeError, TypeError, ValueError):
                continue
    except Exception:
        pass
    
    return urls

async def fetch_via_proxy(client: httpx.AsyncClient, url: str, proxy_base: str):
    """Fetch URL qua proxy"""
    proxy_url = url.replace(ORIGIN, proxy_base)
    
    try:
        r = await client.get(
            proxy_url, 
            headers={"User-Agent": UA, "Accept-Encoding": "identity"}, 
            timeout=30.0,
            follow_redirects=True
        )
        return r
    except httpx.ConnectError as e:
        error_msg = f"Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi proxy {proxy_base}. H√£y ƒë·∫£m b·∫£o proxy ƒëang ch·∫°y!"
        print(f"[CONNECTION_ERROR] {url}: {error_msg}")
        raise Exception(error_msg) from e
    except httpx.HTTPError as e:
        print(f"[HTTP_ERROR] {url}: {e}")
        raise
    except Exception as e:
        print(f"[ERROR] {url}: {e}")
        raise

async def cache_urls(urls: list, proxy_base: str, concurrency: int = 10):
    """
    T·ª± ƒë·ªông cache c√°c URLs t√¨m ƒë∆∞·ª£c
    """
    if not urls:
        return
    
    print(f"\nüì¶ B·∫Øt ƒë·∫ßu cache {len(urls)} URLs...")
    
    async with httpx.AsyncClient() as client:
        sem = asyncio.Semaphore(concurrency)
        cached_count = 0
        error_count = 0
        
        async def cache_one(url):
            nonlocal cached_count, error_count
            async with sem:
                try:
                    r = await fetch_via_proxy(client, url, proxy_base)
                    if r.status_code == 200:
                        cached_count += 1
                        if cached_count % 10 == 0:
                            print(f"  ‚úÖ ƒê√£ cache {cached_count}/{len(urls)} URLs...")
                    else:
                        error_count += 1
                        print(f"  ‚ö†Ô∏è  [{r.status_code}] {url}")
                except Exception as e:
                    error_count += 1
                    print(f"  ‚ùå L·ªói khi cache {url}: {e}")
                await asyncio.sleep(0.3)  # Delay gi·ªØa c√°c requests
        
        tasks = [cache_one(url) for url in urls]
        await asyncio.gather(*tasks)
        
        print(f"\n‚úÖ Ho√†n th√†nh cache:")
        print(f"   - Th√†nh c√¥ng: {cached_count} URLs")
        print(f"   - L·ªói: {error_count} URLs")

def load_important_links() -> list:
    """Load danh s√°ch URLs t·ª´ important_links.json"""
    if not os.path.exists(IMPORTANT_LINKS_FILE):
        return []
    
    try:
        with open(IMPORTANT_LINKS_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if isinstance(data, list):
            return data
        elif isinstance(data, dict) and 'urls' in data:
            return [item.get('url') if isinstance(item, dict) else item for item in data['urls']]
        
        return []
    except Exception as e:
        print(f"‚ö†Ô∏è  L·ªói khi ƒë·ªçc {IMPORTANT_LINKS_FILE}: {e}")
        return []

def save_important_links(urls: list):
    """L∆∞u danh s√°ch URLs v√†o important_links.json"""
    try:
        # L∆∞u d·∫°ng list ƒë∆°n gi·∫£n
        with open(IMPORTANT_LINKS_FILE, 'w', encoding='utf-8') as f:
            json.dump(urls, f, ensure_ascii=False, indent=2)
        print(f"‚úÖ ƒê√£ l∆∞u {len(urls)} URLs v√†o {IMPORTANT_LINKS_FILE}")
    except Exception as e:
        print(f"‚ùå L·ªói khi l∆∞u {IMPORTANT_LINKS_FILE}: {e}")
        raise

async def extract_and_save(url: str, proxy_base: str, auto_cache: bool = True, cache_concurrency: int = 10):
    """
    Extract docId v√† page links t·ª´ URL v√† append v√†o important_links.json
    """
    print(f"üîç ƒêang extract links t·ª´: {url}")
    print(f"   Proxy: {proxy_base}")
    print(f"   Auto cache: {auto_cache}")
    print("")
    
    # Fetch HTML qua proxy
    async with httpx.AsyncClient() as client:
        try:
            print("üì• ƒêang fetch HTML...")
            r = await fetch_via_proxy(client, url, proxy_base)
            
            if r.status_code != 200:
                print(f"‚ùå Kh√¥ng th·ªÉ fetch URL: HTTP {r.status_code}")
                return
            
            # Decode HTML
            ctype = r.headers.get("Content-Type", "").lower()
            enc = re.search(r"charset=([^;]+)", ctype, flags=re.I)
            enc = enc.group(1).strip() if enc else "utf-8"
            
            try:
                html = r.content.decode(enc, errors="replace")
            except Exception:
                html = r.text
            
            print(f"‚úÖ ƒê√£ fetch HTML ({len(html)} chars)")
            print("")
            
        except Exception as e:
            print(f"‚ùå L·ªói khi fetch URL: {e}")
            return
    
    # Extract links c√≥ docId v√† page
    print("üîç ƒêang extract links c√≥ docId v√† page...")
    extracted_urls = extract_docid_page_links(url, html)
    
    if not extracted_urls:
        print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y links n√†o c√≥ docId v√† page")
        return
    
    print(f"‚úÖ T√¨m th·∫•y {len(extracted_urls)} links c√≥ docId v√† page")
    print("")
    
    # Load existing URLs
    existing_urls = set(load_important_links())
    
    # Filter URLs m·ªõi (ch∆∞a c√≥ trong important_links.json)
    new_urls = []
    for u in extracted_urls:
        if u not in existing_urls:
            new_urls.append(u)
            existing_urls.add(u)
    
    if not new_urls:
        print("‚ÑπÔ∏è  T·∫•t c·∫£ links ƒë√£ c√≥ trong important_links.json")
        return
    
    print(f"üìù T√¨m th·∫•y {len(new_urls)} links m·ªõi")
    print("")
    
    # Hi·ªÉn th·ªã m·ªôt v√†i v√≠ d·ª•
    print("üìã V√≠ d·ª• c√°c links m·ªõi:")
    for i, u in enumerate(list(new_urls)[:5], 1):
        print(f"   {i}. {u}")
    if len(new_urls) > 5:
        print(f"   ... v√† {len(new_urls) - 5} links kh√°c")
    print("")
    
    # Append v√†o important_links.json
    all_urls = list(existing_urls)
    all_urls.extend(new_urls)
    all_urls.sort()  # S·∫Øp x·∫øp ƒë·ªÉ d·ªÖ ƒë·ªçc
    
    save_important_links(all_urls)
    
    # T·ª± ƒë·ªông cache c√°c URLs m·ªõi
    if auto_cache:
        await cache_urls(new_urls, proxy_base, cache_concurrency)
    
    print(f"\n{'='*60}")
    print(f"‚úÖ Ho√†n th√†nh!")
    print(f"   - T·ªïng URLs trong {IMPORTANT_LINKS_FILE}: {len(all_urls)}")
    print(f"   - URLs m·ªõi th√™m: {len(new_urls)}")
    print(f"{'='*60}")

def main():
    parser = argparse.ArgumentParser(
        description="Extract docId v√† page links t·ª´ URL v√† append v√†o important_links.json"
    )
    parser.add_argument("url", type=str, help="URL ƒë·ªÉ extract links (v√≠ d·ª•: https://kiagds.ru/?mode=ETM&marke=KM&year=2024&model=8353&mkb=129__25552&docId=434175&page=4)")
    parser.add_argument("--proxy-base", type=str, default=PROXY_BASE, 
                        help=f"Proxy base URL (m·∫∑c ƒë·ªãnh: {PROXY_BASE})")
    parser.add_argument("--no-cache", action="store_true", 
                        help="Kh√¥ng t·ª± ƒë·ªông cache c√°c URLs t√¨m ƒë∆∞·ª£c")
    parser.add_argument("--cache-concurrency", type=int, default=10,
                        help="S·ªë l∆∞·ª£ng requests ƒë·ªìng th·ªùi khi cache (m·∫∑c ƒë·ªãnh: 10)")
    
    args = parser.parse_args()
    
    # Normalize URL
    url = normalize_url(args.url)
    
    if not in_domain(url):
        print(f"‚ùå URL kh√¥ng thu·ªôc domain kiagds.ru: {url}")
        sys.exit(1)
    
    # Ch·∫°y async
    asyncio.run(extract_and_save(
        url, 
        args.proxy_base, 
        auto_cache=not args.no_cache,
        cache_concurrency=args.cache_concurrency
    ))

if __name__ == "__main__":
    main()

