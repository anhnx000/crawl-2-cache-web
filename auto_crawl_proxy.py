#!/usr/bin/env python3
"""
Auto crawler qua proxy ƒë·ªÉ cache to√†n b·ªô trang web
T·ª± ƒë·ªông extract v√† crawl c√°c links trong HTML
"""

import asyncio
import os
import json
import sys
import hashlib
import argparse
import re
from urllib.parse import urlparse, urljoin, urlencode, parse_qs
import httpx
from bs4 import BeautifulSoup

# ================== CONFIG ==================
PROXY_BASE = os.getenv("LOCAL_BASE", "http://localhost:5002")  # Proxy ƒëang ch·∫°y
ORIGIN = os.getenv("ORIGIN", "https://kiagds.ru")
CACHE_DIR = os.getenv("CACHE_DIR", "cache")
UA = "AutoCrawler/1.0 (+respectful; via-proxy)"
os.makedirs(CACHE_DIR, exist_ok=True)
# ============================================

def cache_key(method: str, url: str) -> str:
    """T·∫°o cache key gi·ªëng v·ªõi app.py"""
    return hashlib.sha256(f"{method} {url}".encode("utf-8")).hexdigest()

def cache_paths(method: str, url: str):
    """T·∫°o ƒë∆∞·ªùng d·∫´n cache file gi·ªëng v·ªõi app.py"""
    key = cache_key(method, url)
    return os.path.join(CACHE_DIR, key + ".bin"), os.path.join(CACHE_DIR, key + ".json")

def is_cached(url: str) -> bool:
    """Ki·ªÉm tra URL ƒë√£ ƒë∆∞·ª£c cache ch∆∞a"""
    bin_path, meta_path = cache_paths("GET", url)
    return os.path.exists(bin_path) and os.path.exists(meta_path)

def in_domain(u: str) -> bool:
    """Ki·ªÉm tra URL c√≥ thu·ªôc domain kiagds.ru kh√¥ng"""
    try:
        parsed = urlparse(u)
        netloc = parsed.netloc.lower().split(":")[0]
        return netloc.endswith("kiagds.ru")
    except Exception:
        return False

def normalize_url(url: str) -> str:
    """Chu·∫©n h√≥a URL: lo·∫°i b·ªè fragment, chu·∫©n h√≥a v√† x·ª≠ l√Ω parameters r·ªóng"""
    try:
        parsed = urlparse(url)
        # Lo·∫°i b·ªè fragment (#)
        base_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        
        if parsed.query:
            # Parse v√† l√†m s·∫°ch query parameters
            params = parse_qs(parsed.query, keep_blank_values=False)
            # Lo·∫°i b·ªè c√°c parameters c√≥ gi√° tr·ªã r·ªóng ho·∫∑c ch·ªâ c√≥ '='
            cleaned_params = {}
            for key, values in params.items():
                # Ch·ªâ gi·ªØ l·∫°i c√°c values kh√¥ng r·ªóng
                non_empty_values = [v for v in values if v and v.strip()]
                if non_empty_values:
                    cleaned_params[key] = non_empty_values[0] if len(non_empty_values) == 1 else non_empty_values
            
            # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát: page= ·ªü cu·ªëi URL
            if 'page' in cleaned_params:
                page_val = cleaned_params['page']
                # N·∫øu page l√† chu·ªói r·ªóng ho·∫∑c ch·ªâ c√≥ '=', x√≥a n√≥
                if isinstance(page_val, str) and (not page_val or page_val.strip() == ''):
                    del cleaned_params['page']
                elif isinstance(page_val, list) and all(not v or v.strip() == '' for v in page_val):
                    del cleaned_params['page']
            
            if cleaned_params:
                normalized = f"{base_url}?{urlencode(cleaned_params, doseq=True)}"
            else:
                normalized = base_url
        else:
            normalized = base_url
        
        return normalized
    except Exception:
        # Fallback: lo·∫°i b·ªè fragment v√† parameters r·ªóng ·ªü cu·ªëi
        url_no_fragment = url.split('#')[0]
        # X√≥a &page= ho·∫∑c ?page= ·ªü cu·ªëi
        url_cleaned = url_no_fragment.rstrip('=&?')
        if url_cleaned.endswith('&page') or url_cleaned.endswith('?page'):
            url_cleaned = url_cleaned[:-5]
        return url_cleaned.rstrip('&?')

def extract_pagination_info(base_url: str, html: str):
    """
    Extract th√¥ng tin pagination t·ª´ HTML
    Returns: (max_page, pagination_type)
    - max_page: s·ªë trang t·ªëi ƒëa (None n·∫øu kh√¥ng t√¨m th·∫•y)
    - pagination_type: 'page_of' ho·∫∑c 'numbered' ho·∫∑c None
    """
    soup = BeautifulSoup(html, "html.parser")
    max_page = None
    pagination_type = None
    
    # C√°ch 1: T√¨m "Page X of Y" pattern trong HTML v√† text content
    page_of_pattern = re.search(r'Page\s+(\d+)\s+of\s+(\d+)', html, re.IGNORECASE)
    if not page_of_pattern:
        # T√¨m trong text content c·ªßa soup
        text_content = soup.get_text()
        page_of_pattern = re.search(r'Page\s+(\d+)\s+of\s+(\d+)', text_content, re.IGNORECASE)
    
    if page_of_pattern:
        current_page = int(page_of_pattern.group(1))
        max_page = int(page_of_pattern.group(2))
        pagination_type = 'page_of'
        return max_page, pagination_type
    
    # C√°ch 2: T√¨m pagination links (li·ªát k√™ s·ªë trang)
    page_numbers = set()
    
    # T√¨m trong onclick handlers
    for el in soup.find_all(attrs=lambda x: x and isinstance(x, dict) and 'onclick' in x):
        onclick = el.get('onclick', '')
        if 'page=' in onclick:
            matches = re.findall(r'[&?]page=(\d+)', onclick)
            page_numbers.update(int(m) for m in matches if m.isdigit())
    
    # T√¨m trong href attributes
    for el in soup.find_all(['a', 'link']):
        href = el.get('href', '')
        if href and 'page=' in href:
            matches = re.findall(r'[&?]page=(\d+)', href)
            page_numbers.update(int(m) for m in matches if m.isdigit())
    
    # T√¨m trong text content c√≥ ch·ª©a ¬´ v√† ¬ª (pagination navigation)
    pagination_text = soup.find(string=re.compile(r'[¬´¬ª]', re.I))
    if pagination_text:
        # T√¨m c√°c s·ªë trong pagination container
        parent = pagination_text.parent
        if parent:
            container_text = parent.get_text()
            # T√¨m c√°c s·ªë trong context pagination
            text_numbers = re.findall(r'\b(\d+)\b', container_text)
            # L·ªçc c√°c s·ªë c√≥ th·ªÉ l√† s·ªë trang (th∆∞·ªùng l√† s·ªë nh·ªè ho·∫∑c trong context pagination)
            for num_str in text_numbers:
                num = int(num_str)
                if 1 <= num <= 1000:  # Gi·ªõi h·∫°n h·ª£p l√Ω cho s·ªë trang
                    page_numbers.add(num)
    
    # T√¨m c√°c element c√≥ text l√† s·ªë v√† c√≥ onclick/href v·ªõi page=
    for el in soup.find_all(['a', 'span', 'div', 'li', 'button']):
        text = el.get_text(strip=True)
        if text.isdigit():
            onclick = el.get('onclick', '')
            href = el.get('href', '')
            if 'page=' in onclick or 'page=' in href:
                try:
                    page_num = int(text)
                    if 1 <= page_num <= 1000:
                        page_numbers.add(page_num)
                except ValueError:
                    pass
    
    if page_numbers:
        max_page = max(page_numbers)
        pagination_type = 'numbered'
        return max_page, pagination_type
    
    return None, None

def extract_links(base_url: str, html: str):
    """Extract t·∫•t c·∫£ links t·ª´ HTML"""
    soup = BeautifulSoup(html, "html.parser")
    urls = set()
    
    # Extract t·ª´ c√°c th·∫ª HTML
    for tag, attr in (
        ("a", "href"), 
        ("link", "href"), 
        ("script", "src"), 
        ("img", "src"), 
        ("source", "src"), 
        ("iframe", "src"),
        ("form", "action")
    ):
        for el in soup.find_all(tag):
            href = el.get(attr)
            if not href:
                continue
            try:
                u = urljoin(base_url, href)
                if in_domain(u):
                    u = normalize_url(u)
                    urls.add(u)
            except Exception:
                continue
    
    # Extract t·ª´ JavaScript (t√¨m c√°c URL trong JS)
    for script in soup.find_all("script"):
        if script.string:
            # T√¨m c√°c URL kiagds.ru trong JavaScript
            js_urls = re.findall(r'https?://kiagds\.ru[^\s"\'<>)]+', script.string)
            for u in js_urls:
                try:
                    u = normalize_url(u)
                    if in_domain(u):
                        urls.add(u)
                except Exception:
                    continue
    
    # Extract t·ª´ c√°c thu·ªôc t√≠nh data-* v√† onclick handlers
    try:
        for el in soup.find_all():
            try:
                if not hasattr(el, 'attrs') or not el.attrs:
                    continue
                # BeautifulSoup c√≥ th·ªÉ tr·∫£ v·ªÅ attrs l√† dict ho·∫∑c list
                attrs = el.attrs
                if not isinstance(attrs, dict):
                    continue
                
                for attr_name, attr_value in attrs.items():
                    if not isinstance(attr_value, str):
                        continue
                    
                    # Extract t·ª´ data-* attributes c√≥ ch·ª©a URL
                    if attr_name.startswith('data-') and 'kiagds.ru' in attr_value:
                        js_urls = re.findall(r'https?://kiagds\.ru[^\s"\'<>)]+', attr_value)
                        for u in js_urls:
                            try:
                                u = normalize_url(u)
                                if in_domain(u):
                                    urls.add(u)
                            except Exception:
                                continue
                    
                    # Extract t·ª´ onclick handlers (ajaxHref, location.href, etc.)
                    if attr_name == 'onclick' and ('docId=' in attr_value or 'kiagds.ru' in attr_value):
                        # T√¨m ajaxHref('...') ho·∫∑c c√°c h√†m t∆∞∆°ng t·ª±
                        onclick_urls = re.findall(r"(?:ajaxHref|location\.href|window\.location)\s*[=\(]\s*['\"]([^'\"]+)['\"]", attr_value)
                        for match in onclick_urls:
                            try:
                                # N·∫øu l√† relative URL, join v·ªõi base_url
                                if match.startswith('?'):
                                    u = urljoin(base_url, match)
                                elif match.startswith('/'):
                                    u = urljoin(base_url, match)
                                elif 'kiagds.ru' in match:
                                    u = match
                                else:
                                    u = urljoin(base_url, match)
                                
                                u = normalize_url(u)
                                if in_domain(u):
                                    urls.add(u)
                            except Exception:
                                continue
                        
                        # C≈©ng t√¨m c√°c URL pattern tr·ª±c ti·∫øp trong onclick
                        direct_urls = re.findall(r'https?://kiagds\.ru[^\s"\'<>)]+', attr_value)
                        for u in direct_urls:
                            try:
                                u = normalize_url(u)
                                if in_domain(u):
                                    urls.add(u)
                            except Exception:
                                continue
                    
                    # Extract t·ª´ c√°c attribute kh√°c c√≥ ch·ª©a docId ho·∫∑c URL
                    if 'docId=' in attr_value or 'kiagds.ru' in attr_value:
                        # T√¨m c√°c query string pattern v·ªõi docId
                        query_urls = re.findall(r'\?mode=[^\s"\'<>)]+docId=\d+', attr_value)
                        for qs in query_urls:
                            try:
                                u = urljoin(base_url, qs)
                                u = normalize_url(u)
                                if in_domain(u):
                                    urls.add(u)
                            except Exception:
                                continue
                        
                        # T√¨m c√°c URL ƒë·∫ßy ƒë·ªß
                        full_urls = re.findall(r'https?://kiagds\.ru[^\s"\'<>)]+', attr_value)
                        for u in full_urls:
                            try:
                                u = normalize_url(u)
                                if in_domain(u):
                                    urls.add(u)
                            except Exception:
                                continue
                                
            except (AttributeError, TypeError, ValueError):
                continue
    except Exception:
        pass  # B·ªè qua n·∫øu c√≥ l·ªói khi extract attributes
    
    return urls

async def fetch_via_proxy(client: httpx.AsyncClient, url: str, proxy_base: str):
    """Fetch URL qua proxy"""
    # Chuy·ªÉn ƒë·ªïi URL origin sang proxy URL
    proxy_url = url.replace(ORIGIN, proxy_base)
    
    try:
        r = await client.get(
            proxy_url, 
            headers={"User-Agent": UA, "Accept-Encoding": "identity"}, 
            timeout=30.0,
            follow_redirects=True
        )
        return r
    except httpx.ConnectError as e:
        error_msg = f"Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi proxy {proxy_base}. H√£y ƒë·∫£m b·∫£o proxy ƒëang ch·∫°y!"
        print(f"[CONNECTION_ERROR] {url}: {error_msg}")
        raise Exception(error_msg) from e
    except httpx.HTTPError as e:
        print(f"[HTTP_ERROR] {url}: {e}")
        raise
    except Exception as e:
        print(f"[ERROR] {url}: {e}")
        raise

async def fetch_via_proxy_with_retry(client: httpx.AsyncClient, url: str, proxy_base: str, max_retries: int = 10, verbose: bool = False):
    """Fetch URL qua proxy v·ªõi retry logic"""
    last_exception = None
    
    for attempt in range(max_retries):
        try:
            r = await fetch_via_proxy(client, url, proxy_base)
            
            # N·∫øu th√†nh c√¥ng ·ªü l·∫ßn retry th·ª© 2 tr·ªü ƒëi, log ra
            if attempt > 0:
                print(f"  ‚úÖ Retry th√†nh c√¥ng sau {attempt + 1} l·∫ßn th·ª≠: {url}")
            
            return r
        except Exception as e:
            last_exception = e
            
            # N·∫øu l√† l·ªói connection (proxy kh√¥ng ch·∫°y), kh√¥ng retry
            if "Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi proxy" in str(e):
                raise
            
            # Log retry attempt
            if attempt < max_retries - 1:
                retry_delay = min(2 ** attempt, 10)  # Exponential backoff, max 10s
                if verbose or attempt > 2:  # Ch·ªâ log t·ª´ l·∫ßn th·ª≠ th·ª© 3 ho·∫∑c khi verbose
                    print(f"  ‚ö†Ô∏è  L·∫ßn th·ª≠ {attempt + 1}/{max_retries} th·∫•t b·∫°i: {url}")
                    print(f"     L·ªói: {type(e).__name__}: {str(e)[:100]}")
                    print(f"     ƒê·ª£i {retry_delay}s tr∆∞·ªõc khi retry...")
                await asyncio.sleep(retry_delay)
            else:
                # H·∫øt s·ªë l·∫ßn retry
                print(f"  ‚ùå ƒê√£ retry {max_retries} l·∫ßn nh∆∞ng v·∫´n th·∫•t b·∫°i: {url}")
    
    # Raise exception cu·ªëi c√πng n·∫øu t·∫•t c·∫£ l·∫ßn retry ƒë·ªÅu th·∫•t b·∫°i
    raise last_exception

async def crawl(args, proxy_base: str):
    seen = set()
    q = asyncio.Queue()
    cached_count = 0
    new_count = 0
    error_count = 0

    # Th√™m seed URLs
    seeds = []
    if args.seed:
        for p in range(args.start_page, args.end_page + 1):
            seed_url = f"{args.seed}{p}"
            normalized = normalize_url(seed_url)
            seeds.append(normalized)
    
    if args.extra_urls:
        for u in args.extra_urls:
            normalized = normalize_url(u)
            seeds.append(normalized)
    
    if not seeds:
        # Default: trang ch·ªß qua proxy
        seeds = [f"{proxy_base}/"]

    for u in seeds:
        normalized = normalize_url(u)
        if normalized not in seen:
            await q.put((normalized, 0))
            seen.add(normalized)

    if not seeds:
        print("‚ùå Kh√¥ng c√≥ seed URL n√†o!")
        return

    # Ki·ªÉm tra proxy c√≥ ƒëang ch·∫°y kh√¥ng
    print(f"\nüîç Ki·ªÉm tra proxy {proxy_base}...")
    proxy_ok = False
    try:
        test_client = httpx.Client(timeout=5.0)
        test_response = test_client.get(f"{proxy_base}/_cache_stats")
        if test_response.status_code == 200:
            print(f"‚úÖ Proxy ƒëang ch·∫°y")
            try:
                stats = test_response.json()
                print(f"   - Cached responses: {stats.get('cached_responses', 'N/A')}")
                print(f"   - Live fallback: {stats.get('live_fallback', 'N/A')}")
            except:
                pass
            proxy_ok = True
        else:
            print(f"‚ö†Ô∏è  Proxy tr·∫£ v·ªÅ status {test_response.status_code}")
        test_client.close()
    except Exception as e:
        print(f"‚ùå Proxy kh√¥ng th·ªÉ k·∫øt n·ªëi: {e}")
        print(f"\nüí° H√£y ch·∫°y proxy tr∆∞·ªõc trong terminal kh√°c:")
        print(f"   conda activate crawl")
        print(f"   export LIVE_FALLBACK=true")
        print(f"   conda run -n crawl python app.py")
        print(f"\n‚ö†Ô∏è  Kh√¥ng th·ªÉ ti·∫øp t·ª•c crawl n·∫øu proxy kh√¥ng ch·∫°y!")
        return
    
    if not proxy_ok:
        print(f"\n‚ö†Ô∏è  Proxy check kh√¥ng th√†nh c√¥ng. D·ª´ng crawl ƒë·ªÉ tr√°nh l·ªói.")
        return

    print(f"\nüìã Seed URLs: {len(seeds)}")
    for seed in seeds[:5]:  # Hi·ªÉn th·ªã 5 seed ƒë·∫ßu
        print(f"   - {seed}")
    if len(seeds) > 5:
        print(f"   ... v√† {len(seeds) - 5} URL kh√°c")

    limits = httpx.Limits(max_keepalive_connections=10, max_connections=args.concurrency)
    timeout = httpx.Timeout(30.0)
    
    async with httpx.AsyncClient(limits=limits, timeout=timeout) as client:
        sem = asyncio.Semaphore(args.concurrency)

        async def worker():
            nonlocal cached_count, new_count, error_count
            while True:
                try:
                    url, depth = await asyncio.wait_for(q.get(), timeout=1.0)
                except asyncio.TimeoutError:
                    return
                
                # Ki·ªÉm tra ƒë√£ cache ch∆∞a
                if is_cached(url):
                    cached_count += 1
                    if args.verbose:
                        print(f"[CACHED] {url}")
                    q.task_done()
                    continue

                try:
                    async with sem:
                        if args.delay > 0:
                            await asyncio.sleep(args.delay)
                        
                        r = await fetch_via_proxy_with_retry(client, url, proxy_base, max_retries=args.max_retries, verbose=args.verbose)
                        new_count += 1
                        status_icon = "‚úÖ" if r.status_code == 200 else "‚ö†Ô∏è"
                        print(f"{status_icon} [{r.status_code}] {url} (depth={depth})")
                        
                        # Extract links t·ª´ HTML
                        ctype = r.headers.get("Content-Type", "").lower()
                        if args.follow_depth > depth and ("text/html" in ctype or "application/xhtml" in ctype):
                            try:
                                enc = re.search(r"charset=([^;]+)", ctype, flags=re.I)
                                enc = enc.group(1).strip() if enc else "utf-8"
                                text = r.content.decode(enc, errors="replace")
                            except Exception:
                                try:
                                    text = r.text
                                except Exception:
                                    text = r.content.decode("utf-8", errors="replace")
                            
                            # Extract links v·ªõi error handling
                            try:
                                links = extract_links(url, text)
                                added = 0
                                for link in links:
                                    try:
                                        normalized = normalize_url(link)
                                        if in_domain(normalized) and normalized not in seen:
                                            seen.add(normalized)
                                            await q.put((normalized, depth + 1))
                                            added += 1
                                    except Exception:
                                        continue
                                
                                # T·ª± ƒë·ªông ph√°t hi·ªán v√† crawl pagination
                                if args.auto_pagination:
                                    try:
                                        max_page, pagination_type = extract_pagination_info(url, text)
                                        if max_page and max_page > 1:
                                            print(f"  üìÑ Ph√°t hi·ªán pagination: {pagination_type}, max_page={max_page}")
                                            
                                            # T·∫°o base URL cho pagination
                                            parsed = urlparse(url)
                                            query_params = {}
                                            if parsed.query:
                                                query_params = {k: v[0] if len(v) == 1 else v for k, v in parse_qs(parsed.query).items()}
                                            
                                            # X√≥a page parameter n·∫øu c√≥ ƒë·ªÉ t·∫°o base URL
                                            base_url_for_pagination = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
                                            
                                            # Th√™m t·∫•t c·∫£ c√°c trang v√†o queue
                                            pagination_added = 0
                                            for page_num in range(1, max_page + 1):
                                                query_params['page'] = str(page_num)
                                                pagination_url = f"{base_url_for_pagination}?{urlencode(query_params)}"
                                                normalized = normalize_url(pagination_url)
                                                
                                                if normalized not in seen:
                                                    seen.add(normalized)
                                                    await q.put((normalized, depth))
                                                    pagination_added += 1
                                            
                                            if pagination_added > 0:
                                                print(f"  üìÑ ƒê√£ th√™m {pagination_added} trang pagination v√†o queue")
                                    except Exception as pagination_error:
                                        if args.verbose:
                                            print(f"  ‚ö†Ô∏è  Kh√¥ng th·ªÉ extract pagination t·ª´ {url}: {pagination_error}")
                                
                                if args.verbose and added > 0:
                                    print(f"  üîó Found {added} new links (total: {len(seen)})")
                            except Exception as extract_error:
                                # Log nh∆∞ng kh√¥ng crash n·∫øu extract links fail
                                if args.verbose:
                                    print(f"  ‚ö†Ô∏è  Kh√¥ng th·ªÉ extract links t·ª´ {url}: {extract_error}")
                            
                except httpx.HTTPError as e:
                    error_count += 1
                    print(f"‚ùå [HTTP_ERROR] {url}: {e}")
                except Exception as e:
                    error_count += 1
                    print(f"‚ùå [ERROR] {url}: {e}")
                finally:
                    q.task_done()

        workers = [asyncio.create_task(worker()) for _ in range(args.concurrency)]
        await q.join()
        for w in workers:
            w.cancel()
        
        print(f"\n{'='*60}")
        print(f"‚úÖ Ho√†n th√†nh crawl!")
        print(f"   - ƒê√£ cache s·∫µn: {cached_count} URLs")
        print(f"   - M·ªõi crawl: {new_count} URLs")
        print(f"   - L·ªói: {error_count} URLs")
        print(f"   - T·ªïng URLs ƒë√£ x·ª≠ l√Ω: {len(seen)} URLs")
        print(f"{'='*60}")

if __name__ == "__main__":
    ap = argparse.ArgumentParser(
        description="Auto crawler qua proxy ƒë·ªÉ cache to√†n b·ªô trang web kiagds.ru"
    )
    ap.add_argument("--seed", type=str, default="", 
                    help="Seed URL c√≥ ph·∫ßn &page= (vd: 'https://kiagds.ru/?...&page=')")
    ap.add_argument("--start-page", type=int, default=1,
                    help="Trang b·∫Øt ƒë·∫ßu (n·∫øu seed c√≥ pagination)")
    ap.add_argument("--end-page", type=int, default=1,
                    help="Trang k·∫øt th√∫c (n·∫øu seed c√≥ pagination)")
    ap.add_argument("--extra-urls", nargs="*", help="C√°c URL b·ªï sung", default=[])
    ap.add_argument("--json-file", type=str, default="",
                    help="ƒê∆∞·ªùng d·∫´n file JSON ch·ª©a danh s√°ch URLs ƒë·ªÉ crawl")
    ap.add_argument("--json-start-index", type=int, default=0,
                    help="Ch·ªâ s·ªë b·∫Øt ƒë·∫ßu khi ƒë·ªçc t·ª´ JSON (m·∫∑c ƒë·ªãnh: 0)")
    ap.add_argument("--json-end-index", type=int, default=None,
                    help="Ch·ªâ s·ªë k·∫øt th√∫c khi ƒë·ªçc t·ª´ JSON (m·∫∑c ƒë·ªãnh: t·∫•t c·∫£)")
    ap.add_argument("--concurrency", type=int, default=4, 
                    help="S·ªë l∆∞·ª£ng request ƒë·ªìng th·ªùi (m·∫∑c ƒë·ªãnh: 4)")
    ap.add_argument("--delay", type=float, default=0.5, 
                    help="Gi√£n c√°ch gi·ªØa c√°c request - gi√¢y (m·∫∑c ƒë·ªãnh: 0.5)")
    ap.add_argument("--follow-depth", type=int, default=3, 
                    help="ƒê·ªô s√¢u crawl links (0 = ch·ªâ seeds, >0 = t·ª± ƒë·ªông crawl links, m·∫∑c ƒë·ªãnh: 3)")
    ap.add_argument("--verbose", action="store_true", 
                    help="Hi·ªÉn th·ªã chi ti·∫øt (cached URLs v√† links found)")
    ap.add_argument("--proxy-base", type=str, default="",
                    help="URL proxy base (m·∫∑c ƒë·ªãnh: http://localhost:5002)")
    ap.add_argument("--auto-pagination", action="store_true", default=True,
                    help="T·ª± ƒë·ªông ph√°t hi·ªán v√† crawl pagination (m·∫∑c ƒë·ªãnh: True)")
    ap.add_argument("--max-retries", type=int, default=10,
                    help="S·ªë l·∫ßn retry khi g·∫∑p l·ªói (m·∫∑c ƒë·ªãnh: 10)")
    args = ap.parse_args()
    
    # Load URLs t·ª´ file JSON n·∫øu ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
    if args.json_file:
        try:
            print(f"üìñ ƒêang ƒë·ªçc URLs t·ª´ file: {args.json_file}")
            with open(args.json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            urls_from_json = []
            if isinstance(data, dict):
                if 'urls' in data:
                    # Format: {"urls": [{"url": "..."}, ...]}
                    for item in data['urls']:
                        url = item.get('url') if isinstance(item, dict) else item
                        if url and isinstance(url, str):
                            urls_from_json.append(url)
                elif 'url' in data:
                    # Single URL object
                    urls_from_json.append(data['url'])
            elif isinstance(data, list):
                # Format: [{"url": "..."}, ...] ho·∫∑c ["url1", "url2", ...]
                for item in data:
                    url = item.get('url') if isinstance(item, dict) else item
                    if url and isinstance(url, str):
                        urls_from_json.append(url)
            
            # Apply index range
            total_urls = len(urls_from_json)
            end_idx = args.json_end_index if args.json_end_index is not None else total_urls
            urls_from_json = urls_from_json[args.json_start_index:end_idx]
            
            print(f"‚úÖ ƒê√£ load {len(urls_from_json)} URLs t·ª´ JSON")
            if total_urls > len(urls_from_json):
                print(f"   (t·ª´ index {args.json_start_index} ƒë·∫øn {end_idx-1} trong t·ªïng {total_urls} URLs)")
            if len(urls_from_json) > 1000:
                print(f"‚ö†Ô∏è  S·ªë l∆∞·ª£ng URLs l·ªõn ({len(urls_from_json)}). C√¢n nh·∫Øc s·ª≠ d·ª•ng --json-start-index v√† --json-end-index ƒë·ªÉ chia nh·ªè.")
            
            # Th√™m v√†o extra_urls
            if args.extra_urls:
                args.extra_urls.extend(urls_from_json)
            else:
                args.extra_urls = urls_from_json
                
        except FileNotFoundError:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {args.json_file}")
            sys.exit(1)
        except json.JSONDecodeError as e:
            print(f"‚ùå L·ªói parse JSON: {e}")
            sys.exit(1)
        except Exception as e:
            print(f"‚ùå L·ªói khi ƒë·ªçc file JSON: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
    
    # Override proxy base n·∫øu ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
    proxy_base = PROXY_BASE
    if args.proxy_base:
        proxy_base = args.proxy_base
    
    print(f"{'='*60}")
    print(f"üöÄ Auto Crawler qua Proxy")
    print(f"{'='*60}")
    print(f"   Proxy: {proxy_base}")
    print(f"   Origin: {ORIGIN}")
    print(f"   Cache dir: {CACHE_DIR}")
    print(f"   Follow depth: {args.follow_depth}")
    print(f"   Concurrency: {args.concurrency}")
    print(f"   Delay: {args.delay}s")
    print(f"   Max retries: {args.max_retries}")
    print(f"{'='*60}\n")
    
    asyncio.run(crawl(args, proxy_base))

