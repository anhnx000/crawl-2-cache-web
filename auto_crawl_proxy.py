#!/usr/bin/env python3
"""
Auto crawler qua proxy ƒë·ªÉ cache to√†n b·ªô trang web
T·ª± ƒë·ªông extract v√† crawl c√°c links trong HTML
"""

import asyncio
import os
import json
import sys
import hashlib
import argparse
import re
from typing import Set
from urllib.parse import urlparse, urljoin, urlencode, parse_qs
import httpx
from bs4 import BeautifulSoup

# ================== CONFIG ==================
PROXY_BASE = os.getenv("LOCAL_BASE", "http://localhost:5002")  # Proxy ƒëang ch·∫°y
ORIGIN = os.getenv("ORIGIN", "https://kiagds.ru")
CACHE_DIR = os.getenv("CACHE_DIR", "cache")
IMPORTANT_LINKS_FILE = "important_links.json"
UA = "AutoCrawler/1.0 (+respectful; via-proxy)"
os.makedirs(CACHE_DIR, exist_ok=True)
# ============================================

def cache_key(method: str, url: str) -> str:
    """T·∫°o cache key gi·ªëng v·ªõi app.py"""
    return hashlib.sha256(f"{method} {url}".encode("utf-8")).hexdigest()

def cache_paths(method: str, url: str):
    """T·∫°o ƒë∆∞·ªùng d·∫´n cache file gi·ªëng v·ªõi app.py"""
    key = cache_key(method, url)
    return os.path.join(CACHE_DIR, key + ".bin"), os.path.join(CACHE_DIR, key + ".json")

def is_cached(url: str) -> bool:
    """Ki·ªÉm tra URL ƒë√£ ƒë∆∞·ª£c cache ch∆∞a"""
    bin_path, meta_path = cache_paths("GET", url)
    return os.path.exists(bin_path) and os.path.exists(meta_path)

def has_docid_and_page(url: str) -> bool:
    """Ki·ªÉm tra URL c√≥ ch·ª©a docId v√† page kh√¥ng"""
    try:
        parsed = urlparse(url)
        query = parsed.query
        return 'docId=' in query and 'page=' in query
    except Exception:
        return False

def extract_docids_from_html(base_url: str, html: str) -> Set[str]:
    """
    Extract t·∫•t c·∫£ docId t·ª´ HTML (t·ª´ ajaxHref, href, docid attribute)
    Returns: Set of docId values (strings)
    """
    soup = BeautifulSoup(html, "html.parser")
    docids = set()
    
    # Extract t·ª´ docid attribute (lowercase)
    for el in soup.find_all(attrs={"docid": True}):
        docid = el.get("docid")
        if docid and docid.isdigit():
            docids.add(docid)
    
    # Extract t·ª´ docId trong ajaxHref v√† onclick
    for el in soup.find_all():
        onclick = el.get("onclick", "")
        href = el.get("href", "")
        
        # T√¨m docId trong ajaxHref('...')
        for text in [onclick, href]:
            if not text:
                continue
            
            # Pattern: ajaxHref('?...&docId=123...')
            ajax_matches = re.findall(r"ajaxHref\s*\(\s*['\"]([^'\"]*docId=(\d+)[^'\"]*)['\"]", text, re.IGNORECASE)
            for full_match, docid in ajax_matches:
                if docid.isdigit():
                    docids.add(docid)
            
            # Pattern: docId=123 trong query string
            docid_matches = re.findall(r'[?&]docId=(\d+)', text, re.IGNORECASE)
            for docid in docid_matches:
                if docid.isdigit():
                    docids.add(docid)
    
    return docids

def build_docid_page_urls(base_url: str, docids: Set[str], max_page: int = 10) -> Set[str]:
    """
    T·∫°o c√°c URLs v·ªõi docId v√† page t·ª´ base URL
    Args:
        base_url: URL g·ªëc (v√≠ d·ª•: https://kiagds.ru/?mode=ETM&marke=KM&year=2026&model=9923&mkb=445__29519)
        docids: Set c√°c docId
        max_page: S·ªë trang t·ªëi ƒëa ƒë·ªÉ t·∫°o (m·∫∑c ƒë·ªãnh: 10)
    Returns: Set of URLs
    """
    urls = set()
    
    try:
        parsed = urlparse(base_url)
        base_params = parse_qs(parsed.query, keep_blank_values=False)
        
        # X√≥a docId v√† page n·∫øu c√≥ trong base_params
        base_params.pop("docId", None)
        base_params.pop("docid", None)
        base_params.pop("page", None)
        
        # Build base URL kh√¥ng c√≥ docId v√† page
        base_query = urlencode({k: v[0] if isinstance(v, list) and len(v) == 1 else v 
                               for k, v in base_params.items()}, doseq=True)
        base_url_clean = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        if base_query:
            base_url_clean = f"{base_url_clean}?{base_query}"
        
        # T·∫°o URLs cho m·ªói docId v·ªõi c√°c page
        for docid in docids:
            for page in range(1, max_page + 1):
                # Build URL v·ªõi docId v√† page
                params = {k: (v[0] if isinstance(v, list) and len(v) == 1 else v) 
                         for k, v in base_params.items()}
                params["docId"] = docid
                params["page"] = str(page)
                
                url = f"{base_url_clean}?{urlencode(params)}"
                urls.add(normalize_url(url))
    
    except Exception as e:
        print(f"  ‚ö†Ô∏è  L·ªói khi build docId page URLs: {e}")
    
    return urls

def in_domain(u: str) -> bool:
    """Ki·ªÉm tra URL c√≥ thu·ªôc domain kiagds.ru kh√¥ng"""
    try:
        parsed = urlparse(u)
        netloc = parsed.netloc.lower().split(":")[0]
        return netloc.endswith("kiagds.ru")
    except Exception:
        return False

def normalize_url(url: str) -> str:
    """Chu·∫©n h√≥a URL: lo·∫°i b·ªè fragment, chu·∫©n h√≥a v√† x·ª≠ l√Ω parameters r·ªóng"""
    try:
        parsed = urlparse(url)
        # Lo·∫°i b·ªè fragment (#)
        base_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        
        if parsed.query:
            # Parse v√† l√†m s·∫°ch query parameters
            params = parse_qs(parsed.query, keep_blank_values=False)
            # Lo·∫°i b·ªè c√°c parameters c√≥ gi√° tr·ªã r·ªóng ho·∫∑c ch·ªâ c√≥ '='
            cleaned_params = {}
            for key, values in params.items():
                # Ch·ªâ gi·ªØ l·∫°i c√°c values kh√¥ng r·ªóng
                non_empty_values = [v for v in values if v and v.strip()]
                if non_empty_values:
                    cleaned_params[key] = non_empty_values[0] if len(non_empty_values) == 1 else non_empty_values
            
            # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát: page= ·ªü cu·ªëi URL
            if 'page' in cleaned_params:
                page_val = cleaned_params['page']
                # N·∫øu page l√† chu·ªói r·ªóng ho·∫∑c ch·ªâ c√≥ '=', x√≥a n√≥
                if isinstance(page_val, str) and (not page_val or page_val.strip() == ''):
                    del cleaned_params['page']
                elif isinstance(page_val, list) and all(not v or v.strip() == '' for v in page_val):
                    del cleaned_params['page']
            
            if cleaned_params:
                normalized = f"{base_url}?{urlencode(cleaned_params, doseq=True)}"
            else:
                normalized = base_url
        else:
            normalized = base_url
        
        return normalized
    except Exception:
        # Fallback: lo·∫°i b·ªè fragment v√† parameters r·ªóng ·ªü cu·ªëi
        url_no_fragment = url.split('#')[0]
        # X√≥a &page= ho·∫∑c ?page= ·ªü cu·ªëi
        url_cleaned = url_no_fragment.rstrip('=&?')
        if url_cleaned.endswith('&page') or url_cleaned.endswith('?page'):
            url_cleaned = url_cleaned[:-5]
        return url_cleaned.rstrip('&?')

def extract_pagination_info(base_url: str, html: str):
    """
    Extract th√¥ng tin pagination t·ª´ HTML
    Returns: (max_page, pagination_type)
    - max_page: s·ªë trang t·ªëi ƒëa (None n·∫øu kh√¥ng t√¨m th·∫•y)
    - pagination_type: 'page_of' ho·∫∑c 'numbered' ho·∫∑c None
    """
    soup = BeautifulSoup(html, "html.parser")
    max_page = None
    pagination_type = None
    
    # C√°ch 1: T√¨m "Page X of Y" pattern trong HTML v√† text content
    page_of_pattern = re.search(r'Page\s+(\d+)\s+of\s+(\d+)', html, re.IGNORECASE)
    if not page_of_pattern:
        # T√¨m trong text content c·ªßa soup
        text_content = soup.get_text()
        page_of_pattern = re.search(r'Page\s+(\d+)\s+of\s+(\d+)', text_content, re.IGNORECASE)
    
    if page_of_pattern:
        current_page = int(page_of_pattern.group(1))
        max_page = int(page_of_pattern.group(2))
        pagination_type = 'page_of'
        return max_page, pagination_type
    
    # C√°ch 2: T√¨m pagination links (li·ªát k√™ s·ªë trang)
    page_numbers = set()
    
    # T√¨m trong onclick handlers
    for el in soup.find_all(attrs=lambda x: x and isinstance(x, dict) and 'onclick' in x):
        onclick = el.get('onclick', '')
        if 'page=' in onclick:
            matches = re.findall(r'[&?]page=(\d+)', onclick)
            page_numbers.update(int(m) for m in matches if m.isdigit())
    
    # T√¨m trong href attributes
    for el in soup.find_all(['a', 'link']):
        href = el.get('href', '')
        if href and 'page=' in href:
            matches = re.findall(r'[&?]page=(\d+)', href)
            page_numbers.update(int(m) for m in matches if m.isdigit())
    
    # T√¨m trong text content c√≥ ch·ª©a ¬´ v√† ¬ª (pagination navigation)
    pagination_text = soup.find(string=re.compile(r'[¬´¬ª]', re.I))
    if pagination_text:
        # T√¨m c√°c s·ªë trong pagination container
        parent = pagination_text.parent
        if parent:
            container_text = parent.get_text()
            # T√¨m c√°c s·ªë trong context pagination
            text_numbers = re.findall(r'\b(\d+)\b', container_text)
            # L·ªçc c√°c s·ªë c√≥ th·ªÉ l√† s·ªë trang (th∆∞·ªùng l√† s·ªë nh·ªè ho·∫∑c trong context pagination)
            for num_str in text_numbers:
                num = int(num_str)
                if 1 <= num <= 1000:  # Gi·ªõi h·∫°n h·ª£p l√Ω cho s·ªë trang
                    page_numbers.add(num)
    
    # T√¨m c√°c element c√≥ text l√† s·ªë v√† c√≥ onclick/href v·ªõi page=
    for el in soup.find_all(['a', 'span', 'div', 'li', 'button']):
        text = el.get_text(strip=True)
        if text.isdigit():
            onclick = el.get('onclick', '')
            href = el.get('href', '')
            if 'page=' in onclick or 'page=' in href:
                try:
                    page_num = int(text)
                    if 1 <= page_num <= 1000:
                        page_numbers.add(page_num)
                except ValueError:
                    pass
    
    if page_numbers:
        max_page = max(page_numbers)
        pagination_type = 'numbered'
        return max_page, pagination_type
    
    return None, None

def extract_links(base_url: str, html: str):
    """Extract t·∫•t c·∫£ links t·ª´ HTML"""
    soup = BeautifulSoup(html, "html.parser")
    urls = set()
    
    # Extract t·ª´ c√°c th·∫ª HTML
    for tag, attr in (
        ("a", "href"), 
        ("link", "href"), 
        ("script", "src"), 
        ("img", "src"), 
        ("source", "src"), 
        ("iframe", "src"),
        ("form", "action")
    ):
        for el in soup.find_all(tag):
            href = el.get(attr)
            if not href:
                continue
            try:
                u = urljoin(base_url, href)
                if in_domain(u):
                    u = normalize_url(u)
                    urls.add(u)
            except Exception:
                continue
    
    # Extract t·ª´ JavaScript (t√¨m c√°c URL trong JS)
    for script in soup.find_all("script"):
        if script.string:
            # T√¨m c√°c URL kiagds.ru trong JavaScript
            js_urls = re.findall(r'https?://kiagds\.ru[^\s"\'<>)]+', script.string)
            for u in js_urls:
                try:
                    u = normalize_url(u)
                    if in_domain(u):
                        urls.add(u)
                except Exception:
                    continue
    
    # Extract t·ª´ c√°c thu·ªôc t√≠nh data-* v√† onclick handlers
    try:
        for el in soup.find_all():
            try:
                if not hasattr(el, 'attrs') or not el.attrs:
                    continue
                # BeautifulSoup c√≥ th·ªÉ tr·∫£ v·ªÅ attrs l√† dict ho·∫∑c list
                attrs = el.attrs
                if not isinstance(attrs, dict):
                    continue
                
                for attr_name, attr_value in attrs.items():
                    if not isinstance(attr_value, str):
                        continue
                    
                    # Extract t·ª´ data-* attributes c√≥ ch·ª©a URL
                    if attr_name.startswith('data-') and 'kiagds.ru' in attr_value:
                        js_urls = re.findall(r'https?://kiagds\.ru[^\s"\'<>)]+', attr_value)
                        for u in js_urls:
                            try:
                                u = normalize_url(u)
                                if in_domain(u):
                                    urls.add(u)
                            except Exception:
                                continue
                    
                    # Extract t·ª´ onclick handlers (ajaxHref, location.href, etc.)
                    if attr_name == 'onclick' and ('docId=' in attr_value or 'kiagds.ru' in attr_value):
                        # T√¨m ajaxHref('...') ho·∫∑c c√°c h√†m t∆∞∆°ng t·ª±
                        onclick_urls = re.findall(r"(?:ajaxHref|location\.href|window\.location)\s*[=\(]\s*['\"]([^'\"]+)['\"]", attr_value)
                        for match in onclick_urls:
                            try:
                                # N·∫øu l√† relative URL, join v·ªõi base_url
                                if match.startswith('?'):
                                    u = urljoin(base_url, match)
                                elif match.startswith('/'):
                                    u = urljoin(base_url, match)
                                elif 'kiagds.ru' in match:
                                    u = match
                                else:
                                    u = urljoin(base_url, match)
                                
                                u = normalize_url(u)
                                if in_domain(u):
                                    urls.add(u)
                            except Exception:
                                continue
                        
                        # C≈©ng t√¨m c√°c URL pattern tr·ª±c ti·∫øp trong onclick
                        direct_urls = re.findall(r'https?://kiagds\.ru[^\s"\'<>)]+', attr_value)
                        for u in direct_urls:
                            try:
                                u = normalize_url(u)
                                if in_domain(u):
                                    urls.add(u)
                            except Exception:
                                continue
                    
                    # Extract t·ª´ c√°c attribute kh√°c c√≥ ch·ª©a docId ho·∫∑c URL
                    if 'docId=' in attr_value or 'kiagds.ru' in attr_value:
                        # T√¨m c√°c query string pattern v·ªõi docId
                        query_urls = re.findall(r'\?mode=[^\s"\'<>)]+docId=\d+', attr_value)
                        for qs in query_urls:
                            try:
                                u = urljoin(base_url, qs)
                                u = normalize_url(u)
                                if in_domain(u):
                                    urls.add(u)
                            except Exception:
                                continue
                        
                        # T√¨m c√°c URL ƒë·∫ßy ƒë·ªß
                        full_urls = re.findall(r'https?://kiagds\.ru[^\s"\'<>)]+', attr_value)
                        for u in full_urls:
                            try:
                                u = normalize_url(u)
                                if in_domain(u):
                                    urls.add(u)
                            except Exception:
                                continue
                                
            except (AttributeError, TypeError, ValueError):
                continue
    except Exception:
        pass  # B·ªè qua n·∫øu c√≥ l·ªói khi extract attributes
    
    return urls

async def fetch_via_proxy(client: httpx.AsyncClient, url: str, proxy_base: str):
    """Fetch URL qua proxy"""
    # Chuy·ªÉn ƒë·ªïi URL origin sang proxy URL
    proxy_url = url.replace(ORIGIN, proxy_base)
    
    try:
        r = await client.get(
            proxy_url, 
            headers={"User-Agent": UA, "Accept-Encoding": "identity"}, 
            timeout=30.0,
            follow_redirects=True
        )
        return r
    except httpx.ConnectError as e:
        error_msg = f"Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi proxy {proxy_base}. H√£y ƒë·∫£m b·∫£o proxy ƒëang ch·∫°y!"
        print(f"[CONNECTION_ERROR] {url}: {error_msg}")
        raise Exception(error_msg) from e
    except httpx.HTTPError as e:
        print(f"[HTTP_ERROR] {url}: {e}")
        raise
    except Exception as e:
        print(f"[ERROR] {url}: {e}")
        raise

async def fetch_via_proxy_with_retry(client: httpx.AsyncClient, url: str, proxy_base: str, max_retries: int = 10, verbose: bool = False):
    """Fetch URL qua proxy v·ªõi retry logic"""
    last_exception = None
    
    for attempt in range(max_retries):
        try:
            r = await fetch_via_proxy(client, url, proxy_base)
            
            # N·∫øu th√†nh c√¥ng ·ªü l·∫ßn retry th·ª© 2 tr·ªü ƒëi, log ra
            if attempt > 0:
                print(f"  ‚úÖ Retry th√†nh c√¥ng sau {attempt + 1} l·∫ßn th·ª≠: {url}")
            
            return r
        except Exception as e:
            last_exception = e
            
            # N·∫øu l√† l·ªói connection (proxy kh√¥ng ch·∫°y), kh√¥ng retry
            if "Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi proxy" in str(e):
                raise
            
            # Log retry attempt
            if attempt < max_retries - 1:
                retry_delay = min(2 ** attempt, 10)  # Exponential backoff, max 10s
                if verbose or attempt > 2:  # Ch·ªâ log t·ª´ l·∫ßn th·ª≠ th·ª© 3 ho·∫∑c khi verbose
                    print(f"  ‚ö†Ô∏è  L·∫ßn th·ª≠ {attempt + 1}/{max_retries} th·∫•t b·∫°i: {url}")
                    print(f"     L·ªói: {type(e).__name__}: {str(e)[:100]}")
                    print(f"     ƒê·ª£i {retry_delay}s tr∆∞·ªõc khi retry...")
                await asyncio.sleep(retry_delay)
            else:
                # H·∫øt s·ªë l·∫ßn retry
                print(f"  ‚ùå ƒê√£ retry {max_retries} l·∫ßn nh∆∞ng v·∫´n th·∫•t b·∫°i: {url}")
    
    # Raise exception cu·ªëi c√πng n·∫øu t·∫•t c·∫£ l·∫ßn retry ƒë·ªÅu th·∫•t b·∫°i
    raise last_exception

def load_important_links() -> set:
    """Load danh s√°ch URLs t·ª´ important_links.json"""
    if not os.path.exists(IMPORTANT_LINKS_FILE):
        return set()
    
    try:
        with open(IMPORTANT_LINKS_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        urls = set()
        if isinstance(data, list):
            urls = set(data)
        elif isinstance(data, dict) and 'urls' in data:
            urls = {item.get('url') if isinstance(item, dict) else item for item in data['urls']}
        
        return urls
    except Exception as e:
        print(f"‚ö†Ô∏è  L·ªói khi ƒë·ªçc {IMPORTANT_LINKS_FILE}: {e}")
        return set()

def save_important_links(urls: list):
    """L∆∞u danh s√°ch URLs v√†o important_links.json (thread-safe)"""
    try:
        # S·∫Øp x·∫øp ƒë·ªÉ d·ªÖ ƒë·ªçc
        sorted_urls = sorted(urls)
        with open(IMPORTANT_LINKS_FILE, 'w', encoding='utf-8') as f:
            json.dump(sorted_urls, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"‚ö†Ô∏è  L·ªói khi l∆∞u {IMPORTANT_LINKS_FILE}: {e}")

async def append_to_important_links(new_urls: set, existing_urls: set, lock: asyncio.Lock):
    """Append URLs m·ªõi v√†o important_links.json (thread-safe)"""
    if not new_urls:
        return 0
    
    # Filter URLs m·ªõi (ch∆∞a c√≥ trong existing_urls)
    truly_new = {u for u in new_urls if u not in existing_urls}
    if not truly_new:
        return 0
    
    # Update existing_urls
    existing_urls.update(truly_new)
    
    # L∆∞u v√†o file (c·∫ßn lock ƒë·ªÉ tr√°nh race condition)
    async with lock:
        # Load l·∫°i ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng m·∫•t d·ªØ li·ªáu
        current = load_important_links()
        current.update(truly_new)
        save_important_links(list(current))
    
    return len(truly_new)

async def crawl(args, proxy_base: str):
    seen = set()
    q = asyncio.Queue()
    cached_count = 0
    new_count = 0
    error_count = 0
    
    # Load important_links.json ƒë·ªÉ track URLs ƒë√£ c√≥
    important_links = load_important_links()
    print(f"üìñ ƒê√£ load {len(important_links)} URLs t·ª´ {IMPORTANT_LINKS_FILE}")
    
    # Lock ƒë·ªÉ ƒë·∫£m b·∫£o thread-safe khi update important_links.json
    important_links_lock = asyncio.Lock()
    new_important_links_count = 0

    # Th√™m seed URLs
    seeds = []
    if args.seed:
        for p in range(args.start_page, args.end_page + 1):
            seed_url = f"{args.seed}{p}"
            normalized = normalize_url(seed_url)
            seeds.append(normalized)
    
    if args.extra_urls:
        for u in args.extra_urls:
            normalized = normalize_url(u)
            seeds.append(normalized)
    
    if not seeds:
        # Default: trang ch·ªß qua proxy
        seeds = [f"{proxy_base}/"]

    for u in seeds:
        normalized = normalize_url(u)
        if normalized not in seen:
            await q.put((normalized, 0))
            seen.add(normalized)

    if not seeds:
        print("‚ùå Kh√¥ng c√≥ seed URL n√†o!")
        return

    # Ki·ªÉm tra proxy c√≥ ƒëang ch·∫°y kh√¥ng
    print(f"\nüîç Ki·ªÉm tra proxy {proxy_base}...")
    proxy_ok = False
    try:
        test_client = httpx.Client(timeout=5.0)
        test_response = test_client.get(f"{proxy_base}/_cache_stats")
        if test_response.status_code == 200:
            print(f"‚úÖ Proxy ƒëang ch·∫°y")
            try:
                stats = test_response.json()
                print(f"   - Cached responses: {stats.get('cached_responses', 'N/A')}")
                print(f"   - Live fallback: {stats.get('live_fallback', 'N/A')}")
            except:
                pass
            proxy_ok = True
        else:
            print(f"‚ö†Ô∏è  Proxy tr·∫£ v·ªÅ status {test_response.status_code}")
        test_client.close()
    except Exception as e:
        print(f"‚ùå Proxy kh√¥ng th·ªÉ k·∫øt n·ªëi: {e}")
        print(f"\nüí° H√£y ch·∫°y proxy tr∆∞·ªõc trong terminal kh√°c:")
        print(f"   conda activate crawl")
        print(f"   export LIVE_FALLBACK=true")
        print(f"   conda run -n crawl python app.py")
        print(f"\n‚ö†Ô∏è  Kh√¥ng th·ªÉ ti·∫øp t·ª•c crawl n·∫øu proxy kh√¥ng ch·∫°y!")
        return
    
    if not proxy_ok:
        print(f"\n‚ö†Ô∏è  Proxy check kh√¥ng th√†nh c√¥ng. D·ª´ng crawl ƒë·ªÉ tr√°nh l·ªói.")
        return

    print(f"\nüìã Seed URLs: {len(seeds)}")
    for seed in seeds[:5]:  # Hi·ªÉn th·ªã 5 seed ƒë·∫ßu
        print(f"   - {seed}")
    if len(seeds) > 5:
        print(f"   ... v√† {len(seeds) - 5} URL kh√°c")

    limits = httpx.Limits(max_keepalive_connections=10, max_connections=args.concurrency)
    timeout = httpx.Timeout(30.0)
    
    async with httpx.AsyncClient(limits=limits, timeout=timeout) as client:
        sem = asyncio.Semaphore(args.concurrency)

        async def worker():
            nonlocal cached_count, new_count, error_count, new_important_links_count
            while True:
                try:
                    url, depth = await asyncio.wait_for(q.get(), timeout=1.0)
                except asyncio.TimeoutError:
                    return
                
                # Ki·ªÉm tra ƒë√£ cache ch∆∞a (ch·ªâ ƒë·ªÉ ƒë·∫øm, kh√¥ng skip)
                already_cached = is_cached(url)
                if already_cached:
                    cached_count += 1
                    if args.verbose:
                        print(f"[ALREADY_CACHED] {url} (crawl l·∫°i ƒë·ªÉ t√¨m links m·ªõi)")

                try:
                    async with sem:
                        if args.delay > 0:
                            await asyncio.sleep(args.delay)
                        
                        r = await fetch_via_proxy_with_retry(client, url, proxy_base, max_retries=args.max_retries, verbose=args.verbose)
                        # ƒê·∫øm l√† "m·ªõi crawl" n·∫øu ch∆∞a c√≥ cache tr∆∞·ªõc ƒë√≥
                        if not already_cached:
                            new_count += 1
                        status_icon = "‚úÖ" if r.status_code == 200 else "‚ö†Ô∏è"
                        cache_status = " [CACHED]" if already_cached else ""
                        print(f"{status_icon} [{r.status_code}]{cache_status} {url} (depth={depth})")
                        
                        # Extract links t·ª´ HTML
                        ctype = r.headers.get("Content-Type", "").lower()
                        if args.follow_depth > depth and ("text/html" in ctype or "application/xhtml" in ctype):
                            try:
                                enc = re.search(r"charset=([^;]+)", ctype, flags=re.I)
                                enc = enc.group(1).strip() if enc else "utf-8"
                                text = r.content.decode(enc, errors="replace")
                            except Exception:
                                try:
                                    text = r.text
                                except Exception:
                                    text = r.content.decode("utf-8", errors="replace")
                            
                            # Extract links v·ªõi error handling
                            try:
                                links = extract_links(url, text)
                                added = 0
                                new_important_links = set()  # Track URLs c√≥ docId v√† page
                                
                                for link in links:
                                    try:
                                        normalized = normalize_url(link)
                                        if in_domain(normalized) and normalized not in seen:
                                            seen.add(normalized)
                                            await q.put((normalized, depth + 1))
                                            added += 1
                                            
                                            # N·∫øu URL c√≥ docId v√† page, th√™m v√†o danh s√°ch ƒë·ªÉ update important_links.json
                                            if has_docid_and_page(normalized):
                                                new_important_links.add(normalized)
                                    except Exception:
                                        continue
                                
                                # Extract docIds t·ª´ HTML v√† t·∫°o links v·ªõi c√°c page
                                try:
                                    docids = extract_docids_from_html(url, text)
                                    if docids:
                                        # Ph√°t hi·ªán max_page t·ª´ pagination
                                        max_page_info, pagination_type = extract_pagination_info(url, text)
                                        max_page = max_page_info if max_page_info else 10  # Default 10 n·∫øu kh√¥ng t√¨m th·∫•y
                                        
                                        # T·∫°o URLs v·ªõi docId v√† page
                                        docid_page_urls = build_docid_page_urls(url, docids, max_page)
                                        
                                        # Th√™m v√†o queue v√† track
                                        docid_links_added = 0
                                        for docid_url in docid_page_urls:
                                            if docid_url not in seen:
                                                seen.add(docid_url)
                                                await q.put((docid_url, depth + 1))
                                                docid_links_added += 1
                                                new_important_links.add(docid_url)
                                        
                                        if docid_links_added > 0:
                                            print(f"  üîó T√¨m th·∫•y {len(docids)} docId, t·∫°o {docid_links_added} links v·ªõi page (max_page={max_page})")
                                except Exception as docid_error:
                                    if args.verbose:
                                        print(f"  ‚ö†Ô∏è  L·ªói khi extract docIds: {docid_error}")
                                
                                # T·ª± ƒë·ªông update important_links.json n·∫øu c√≥ links m·ªõi c√≥ docId v√† page
                                if new_important_links:
                                    count = await append_to_important_links(new_important_links, important_links, important_links_lock)
                                    if count > 0:
                                        new_important_links_count += count
                                        print(f"  üìù ƒê√£ th√™m {count} URLs c√≥ docId&page v√†o {IMPORTANT_LINKS_FILE} (t·ªïng: {new_important_links_count} m·ªõi)")
                                
                                # T·ª± ƒë·ªông ph√°t hi·ªán v√† crawl pagination
                                if args.auto_pagination:
                                    try:
                                        max_page, pagination_type = extract_pagination_info(url, text)
                                        if max_page and max_page > 1:
                                            print(f"  üìÑ Ph√°t hi·ªán pagination: {pagination_type}, max_page={max_page}")
                                            
                                            # T·∫°o base URL cho pagination
                                            parsed = urlparse(url)
                                            query_params = {}
                                            if parsed.query:
                                                query_params = {k: v[0] if len(v) == 1 else v for k, v in parse_qs(parsed.query).items()}
                                            
                                            # X√≥a page parameter n·∫øu c√≥ ƒë·ªÉ t·∫°o base URL
                                            base_url_for_pagination = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
                                            
                                            # Th√™m t·∫•t c·∫£ c√°c trang v√†o queue
                                            pagination_added = 0
                                            new_pagination_important = set()
                                            
                                            for page_num in range(1, max_page + 1):
                                                query_params['page'] = str(page_num)
                                                pagination_url = f"{base_url_for_pagination}?{urlencode(query_params)}"
                                                normalized = normalize_url(pagination_url)
                                                
                                                if normalized not in seen:
                                                    seen.add(normalized)
                                                    await q.put((normalized, depth))
                                                    pagination_added += 1
                                                    
                                                    # N·∫øu pagination URL c√≥ docId v√† page, th√™m v√†o important_links
                                                    if has_docid_and_page(normalized):
                                                        new_pagination_important.add(normalized)
                                            
                                            if pagination_added > 0:
                                                print(f"  üìÑ ƒê√£ th√™m {pagination_added} trang pagination v√†o queue")
                                            
                                            # Update important_links.json cho pagination URLs
                                            if new_pagination_important:
                                                count = await append_to_important_links(new_pagination_important, important_links, important_links_lock)
                                                if count > 0:
                                                    new_important_links_count += count
                                                    print(f"  üìù ƒê√£ th√™m {count} pagination URLs c√≥ docId&page v√†o {IMPORTANT_LINKS_FILE}")
                                    except Exception as pagination_error:
                                        if args.verbose:
                                            print(f"  ‚ö†Ô∏è  Kh√¥ng th·ªÉ extract pagination t·ª´ {url}: {pagination_error}")
                                
                                if args.verbose and added > 0:
                                    print(f"  üîó Found {added} new links (total: {len(seen)})")
                            except Exception as extract_error:
                                # Log nh∆∞ng kh√¥ng crash n·∫øu extract links fail
                                if args.verbose:
                                    print(f"  ‚ö†Ô∏è  Kh√¥ng th·ªÉ extract links t·ª´ {url}: {extract_error}")
                            
                except httpx.HTTPError as e:
                    error_count += 1
                    print(f"‚ùå [HTTP_ERROR] {url}: {e}")
                except Exception as e:
                    error_count += 1
                    print(f"‚ùå [ERROR] {url}: {e}")
                finally:
                    q.task_done()

        workers = [asyncio.create_task(worker()) for _ in range(args.concurrency)]
        await q.join()
        for w in workers:
            w.cancel()
        
        # ƒê·ª£i t·∫•t c·∫£ c√°c task save important_links.json ho√†n th√†nh
        await asyncio.sleep(1)  # ƒê·ª£i c√°c async save ho√†n th√†nh
        
        print(f"\n{'='*60}")
        print(f"‚úÖ Ho√†n th√†nh crawl!")
        print(f"   - ƒê√£ cache s·∫µn: {cached_count} URLs")
        print(f"   - M·ªõi crawl: {new_count} URLs")
        print(f"   - L·ªói: {error_count} URLs")
        print(f"   - T·ªïng URLs ƒë√£ x·ª≠ l√Ω: {len(seen)} URLs")
        if new_important_links_count > 0:
            print(f"   - URLs c√≥ docId&page m·ªõi th√™m v√†o {IMPORTANT_LINKS_FILE}: {new_important_links_count}")
        print(f"{'='*60}")

if __name__ == "__main__":
    ap = argparse.ArgumentParser(
        description="Auto crawler qua proxy ƒë·ªÉ cache to√†n b·ªô trang web kiagds.ru"
    )
    ap.add_argument("--seed", type=str, default="", 
                    help="Seed URL c√≥ ph·∫ßn &page= (vd: 'https://kiagds.ru/?...&page=')")
    ap.add_argument("--start-page", type=int, default=1,
                    help="Trang b·∫Øt ƒë·∫ßu (n·∫øu seed c√≥ pagination)")
    ap.add_argument("--end-page", type=int, default=1,
                    help="Trang k·∫øt th√∫c (n·∫øu seed c√≥ pagination)")
    ap.add_argument("--extra-urls", nargs="*", help="C√°c URL b·ªï sung", default=[])
    ap.add_argument("--json-file", type=str, default="",
                    help="ƒê∆∞·ªùng d·∫´n file JSON ch·ª©a danh s√°ch URLs ƒë·ªÉ crawl")
    ap.add_argument("--json-start-index", type=int, default=0,
                    help="Ch·ªâ s·ªë b·∫Øt ƒë·∫ßu khi ƒë·ªçc t·ª´ JSON (m·∫∑c ƒë·ªãnh: 0)")
    ap.add_argument("--json-end-index", type=int, default=None,
                    help="Ch·ªâ s·ªë k·∫øt th√∫c khi ƒë·ªçc t·ª´ JSON (m·∫∑c ƒë·ªãnh: t·∫•t c·∫£)")
    ap.add_argument("--concurrency", type=int, default=4, 
                    help="S·ªë l∆∞·ª£ng request ƒë·ªìng th·ªùi (m·∫∑c ƒë·ªãnh: 4)")
    ap.add_argument("--delay", type=float, default=0.5, 
                    help="Gi√£n c√°ch gi·ªØa c√°c request - gi√¢y (m·∫∑c ƒë·ªãnh: 0.5)")
    ap.add_argument("--follow-depth", type=int, default=3, 
                    help="ƒê·ªô s√¢u crawl links (0 = ch·ªâ seeds, >0 = t·ª± ƒë·ªông crawl links, m·∫∑c ƒë·ªãnh: 3)")
    ap.add_argument("--verbose", action="store_true", 
                    help="Hi·ªÉn th·ªã chi ti·∫øt (cached URLs v√† links found)")
    ap.add_argument("--proxy-base", type=str, default="",
                    help="URL proxy base (m·∫∑c ƒë·ªãnh: http://localhost:5002)")
    ap.add_argument("--auto-pagination", action="store_true", default=True,
                    help="T·ª± ƒë·ªông ph√°t hi·ªán v√† crawl pagination (m·∫∑c ƒë·ªãnh: True)")
    ap.add_argument("--max-retries", type=int, default=10,
                    help="S·ªë l·∫ßn retry khi g·∫∑p l·ªói (m·∫∑c ƒë·ªãnh: 10)")
    args = ap.parse_args()
    
    # Load URLs t·ª´ file JSON n·∫øu ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
    if args.json_file:
        try:
            print(f"üìñ ƒêang ƒë·ªçc URLs t·ª´ file: {args.json_file}")
            with open(args.json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            urls_from_json = []
            if isinstance(data, dict):
                if 'urls' in data:
                    # Format: {"urls": [{"url": "..."}, ...]}
                    for item in data['urls']:
                        url = item.get('url') if isinstance(item, dict) else item
                        if url and isinstance(url, str):
                            urls_from_json.append(url)
                elif 'url' in data:
                    # Single URL object
                    urls_from_json.append(data['url'])
            elif isinstance(data, list):
                # Format: [{"url": "..."}, ...] ho·∫∑c ["url1", "url2", ...]
                for item in data:
                    url = item.get('url') if isinstance(item, dict) else item
                    if url and isinstance(url, str):
                        urls_from_json.append(url)
            
            # Apply index range
            total_urls = len(urls_from_json)
            end_idx = args.json_end_index if args.json_end_index is not None else total_urls
            urls_from_json = urls_from_json[args.json_start_index:end_idx]
            
            print(f"‚úÖ ƒê√£ load {len(urls_from_json)} URLs t·ª´ JSON")
            if total_urls > len(urls_from_json):
                print(f"   (t·ª´ index {args.json_start_index} ƒë·∫øn {end_idx-1} trong t·ªïng {total_urls} URLs)")
            if len(urls_from_json) > 1000:
                print(f"‚ö†Ô∏è  S·ªë l∆∞·ª£ng URLs l·ªõn ({len(urls_from_json)}). C√¢n nh·∫Øc s·ª≠ d·ª•ng --json-start-index v√† --json-end-index ƒë·ªÉ chia nh·ªè.")
            
            # Th√™m v√†o extra_urls
            if args.extra_urls:
                args.extra_urls.extend(urls_from_json)
            else:
                args.extra_urls = urls_from_json
                
        except FileNotFoundError:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y file: {args.json_file}")
            sys.exit(1)
        except json.JSONDecodeError as e:
            print(f"‚ùå L·ªói parse JSON: {e}")
            sys.exit(1)
        except Exception as e:
            print(f"‚ùå L·ªói khi ƒë·ªçc file JSON: {e}")
            import traceback
            traceback.print_exc()
            sys.exit(1)
    
    # Override proxy base n·∫øu ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
    proxy_base = PROXY_BASE
    if args.proxy_base:
        proxy_base = args.proxy_base
    
    print(f"{'='*60}")
    print(f"üöÄ Auto Crawler qua Proxy")
    print(f"{'='*60}")
    print(f"   Proxy: {proxy_base}")
    print(f"   Origin: {ORIGIN}")
    print(f"   Cache dir: {CACHE_DIR}")
    print(f"   Follow depth: {args.follow_depth}")
    print(f"   Concurrency: {args.concurrency}")
    print(f"   Delay: {args.delay}s")
    print(f"   Max retries: {args.max_retries}")
    print(f"{'='*60}\n")
    
    asyncio.run(crawl(args, proxy_base))

