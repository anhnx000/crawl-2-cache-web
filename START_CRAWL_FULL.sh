#!/bin/bash
# Script kiá»ƒm tra vÃ  hÆ°á»›ng dáº«n cháº¡y crawl full

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘     ğŸš€ START FULL CRAWL - 16,304 Important Links          â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""

cd /home/xuananh/work_1/anhnx/crawl-2

# BÆ°á»›c 1: Kiá»ƒm tra proxy
echo "ğŸ“‹ Step 1: Kiá»ƒm tra proxy..."
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

if curl -s -o /dev/null -w "%{http_code}" "http://localhost:5002/_cache_stats" | grep -q "200"; then
    echo "   âœ… Proxy Ä‘ang cháº¡y táº¡i http://localhost:5002"
    
    # Láº¥y stats
    STATS=$(curl -s "http://localhost:5002/_cache_stats")
    CACHED=$(echo "$STATS" | python3 -c "import sys, json; print(json.load(sys.stdin).get('cached_responses', 'N/A'))" 2>/dev/null || echo "N/A")
    LIVE=$(echo "$STATS" | python3 -c "import sys, json; print(json.load(sys.stdin).get('live_fallback', 'N/A'))" 2>/dev/null || echo "N/A")
    
    echo "   â€¢ Cached responses: $CACHED"
    echo "   â€¢ Live fallback: $LIVE"
    echo ""
    
    PROXY_OK=true
else
    echo "   âŒ Proxy KHÃ”NG cháº¡y!"
    echo ""
    echo "   ğŸ’¡ HÃ£y má»Ÿ terminal má»›i vÃ  cháº¡y:"
    echo ""
    echo "   cd /home/xuananh/work_1/anhnx/crawl-2"
    echo "   ./start_online.sh"
    echo ""
    echo "   (Giá»¯ terminal Ä‘Ã³ cháº¡y)"
    echo ""
    
    read -p "   Nháº¥n ENTER khi Ä‘Ã£ cháº¡y proxy... " dummy
    
    # Kiá»ƒm tra láº¡i
    if curl -s -o /dev/null -w "%{http_code}" "http://localhost:5002/_cache_stats" | grep -q "200"; then
        echo "   âœ… Proxy Ä‘Ã£ cháº¡y!"
        echo ""
        PROXY_OK=true
    else
        echo "   âŒ Váº«n khÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c proxy"
        echo "   âš ï¸  KhÃ´ng thá»ƒ tiáº¿p tá»¥c!"
        echo ""
        exit 1
    fi
fi

# BÆ°á»›c 2: Kiá»ƒm tra crawler Ä‘ang cháº¡y chÆ°a
echo "ğŸ“‹ Step 2: Kiá»ƒm tra crawler..."
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

if pgrep -f "auto_crawl_proxy.py.*important_links.json" > /dev/null; then
    echo "   âš ï¸  Crawler Ä‘Ã£ Ä‘ang cháº¡y!"
    PID=$(pgrep -f "auto_crawl_proxy.py.*important_links.json")
    echo "   PID: $PID"
    echo ""
    
    read -p "   Báº¡n cÃ³ muá»‘n dá»«ng vÃ  cháº¡y láº¡i? (y/n): " answer
    if [ "$answer" = "y" ] || [ "$answer" = "Y" ]; then
        pkill -f "auto_crawl_proxy.py.*important_links.json"
        echo "   âœ… ÄÃ£ dá»«ng crawler cÅ©"
        sleep 2
    else
        echo "   â„¹ï¸  Giá»¯ nguyÃªn crawler Ä‘ang cháº¡y"
        echo ""
        echo "   ğŸ“Š Xem tiáº¿n trÃ¬nh:"
        echo "      tail -f cache_important_full_depth50_concurrency10.log"
        echo ""
        exit 0
    fi
fi

# BÆ°á»›c 3: Báº¯t Ä‘áº§u crawl
echo ""
echo "ğŸ“‹ Step 3: Báº¯t Ä‘áº§u crawl FULL"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
TOTAL_URLS=$(python3 -c "import json; print(len(json.load(open('important_links.json'))))" 2>/dev/null || echo "16,304")
echo "   Total URLs: $TOTAL_URLS"
echo "   Follow depth: 50"
echo "   Concurrency: 10"
echo "   Delay: 0.3s"
echo "   Log file: cache_important_full_depth50_concurrency10.log"
echo ""

read -p "   Báº¯t Ä‘áº§u crawl? (y/n): " answer
if [ "$answer" != "y" ] && [ "$answer" != "Y" ]; then
    echo "   âš ï¸  ÄÃ£ há»§y"
    exit 0
fi

echo ""
echo "   ğŸš€ Starting crawler..."

nohup python3 auto_crawl_proxy.py \
  --json-file important_links.json \
  --follow-depth 50 \
  --concurrency 10 \
  --delay 0.3 \
  --max-retries 10 \
  --auto-pagination \
  > cache_important_full_depth50_concurrency10.log 2>&1 &

PID=$!
sleep 2

# Kiá»ƒm tra process cÃ³ cháº¡y khÃ´ng
if ps -p $PID > /dev/null; then
    echo "   âœ… Crawler Ä‘Ã£ báº¯t Ä‘áº§u!"
    echo "   PID: $PID"
    echo ""
else
    echo "   âŒ Crawler khÃ´ng khá»Ÿi Ä‘á»™ng Ä‘Æ°á»£c"
    echo "   Kiá»ƒm tra log: cat cache_important_full_depth50_concurrency10.log"
    echo ""
    exit 1
fi

# BÆ°á»›c 4: HÆ°á»›ng dáº«n monitor
echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘                  âœ… CRAWL ÄÃƒ Báº®T Äáº¦U                       â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo ""
echo "ğŸ“Š Monitor tiáº¿n trÃ¬nh:"
echo "   tail -f cache_important_full_depth50_concurrency10.log"
echo "   # hoáº·c"
echo "   watch -n 10 './check_progress.sh'"
echo ""
echo "ğŸ›‘ Dá»«ng crawl:"
echo "   pkill -f auto_crawl_proxy.py"
echo ""
echo "â±ï¸  Æ¯á»›c tÃ­nh thá»i gian: 20-40 giá»"
echo "   (Vá»›i depth=50 vÃ  concurrency=10, sáº½ crawl ráº¥t sÃ¢u vÃ  tÃ¬m nhiá»u links hÆ¡n)"
echo ""
echo "ğŸ’¡ Tip: Má»Ÿ terminal má»›i Ä‘á»ƒ xem log realtime:"
echo "   tail -f cache_important_full_depth50_concurrency10.log"
echo ""

# Hiá»ƒn thá»‹ vÃ i dÃ²ng log Ä‘áº§u
sleep 3
echo "ğŸ“ Log preview (5 giÃ¢y Ä‘áº§u):"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
tail -20 cache_important_full_depth50_concurrency10.log 2>/dev/null || echo "   (Äang khá»Ÿi Ä‘á»™ng...)"
echo ""

