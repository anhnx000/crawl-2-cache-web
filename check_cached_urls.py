#!/usr/bin/env python3
"""
Script kiá»ƒm tra URLs nÃ o trong full_urls_to_crawl.json Ä‘Ã£ Ä‘Æ°á»£c cache
"""

import json
import os
import hashlib
import argparse

CACHE_DIR = os.getenv("CACHE_DIR", "cache")

def cache_key(method: str, url: str) -> str:
    """Táº¡o cache key giá»‘ng vá»›i app.py"""
    return hashlib.sha256(f"{method} {url}".encode("utf-8")).hexdigest()

def is_cached(url: str) -> bool:
    """Kiá»ƒm tra URL Ä‘Ã£ Ä‘Æ°á»£c cache chÆ°a"""
    key = cache_key("GET", url)
    bin_path = os.path.join(CACHE_DIR, key + ".bin")
    meta_path = os.path.join(CACHE_DIR, key + ".json")
    return os.path.exists(bin_path) and os.path.exists(meta_path)

def main():
    ap = argparse.ArgumentParser(
        description="Kiá»ƒm tra URLs nÃ o trong file JSON Ä‘Ã£ Ä‘Æ°á»£c cache"
    )
    ap.add_argument("json_file", nargs="?", default="full_urls_to_crawl.json",
                    help="ÄÆ°á»ng dáº«n file JSON chá»©a URLs (máº·c Ä‘á»‹nh: full_urls_to_crawl.json)")
    ap.add_argument("--output-cached", type=str, default="cached_urls.json",
                    help="File output cho URLs Ä‘Ã£ cache (máº·c Ä‘á»‹nh: cached_urls.json)")
    ap.add_argument("--output-uncached", type=str, default="uncached_urls.json",
                    help="File output cho URLs chÆ°a cache (máº·c Ä‘á»‹nh: uncached_urls.json)")
    ap.add_argument("--show-cached", action="store_true",
                    help="Hiá»ƒn thá»‹ danh sÃ¡ch URLs Ä‘Ã£ cache")
    ap.add_argument("--show-uncached", action="store_true",
                    help="Hiá»ƒn thá»‹ danh sÃ¡ch URLs chÆ°a cache")
    ap.add_argument("--limit", type=int, default=10,
                    help="Sá»‘ lÆ°á»£ng URLs hiá»ƒn thá»‹ (máº·c Ä‘á»‹nh: 10)")
    args = ap.parse_args()
    
    # Äá»c file JSON
    print(f"ğŸ“– Äang Ä‘á»c file: {args.json_file}")
    try:
        with open(args.json_file, "r", encoding="utf-8") as f:
            data = json.load(f)
    except FileNotFoundError:
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file: {args.json_file}")
        return
    except json.JSONDecodeError as e:
        print(f"âŒ Lá»—i parse JSON: {e}")
        return
    
    # Extract URLs
    urls_data = []
    if isinstance(data, dict):
        if 'urls' in data:
            urls_data = data['urls']
        elif 'url' in data:
            urls_data = [data]
    elif isinstance(data, list):
        urls_data = data
    
    total = len(urls_data)
    print(f"âœ… ÄÃ£ load {total} URLs\n")
    
    if total == 0:
        print("âš ï¸  KhÃ´ng cÃ³ URLs nÃ o trong file")
        return
    
    # Kiá»ƒm tra cache
    print("ğŸ” Äang kiá»ƒm tra cache...")
    cached_urls = []
    uncached_urls = []
    
    for i, item in enumerate(urls_data, 1):
        url = item.get("url") if isinstance(item, dict) else item
        if url:
            if is_cached(url):
                cached_urls.append(item)
            else:
                uncached_urls.append(item)
        
        # Progress
        if i % 1000 == 0:
            print(f"   ÄÃ£ kiá»ƒm tra: {i}/{total} URLs...")
    
    # Thá»‘ng kÃª
    print(f"\n{'='*60}")
    print(f"ğŸ“Š Thá»‘ng kÃª:")
    print(f"   - Tá»•ng URLs: {total}")
    print(f"   - ÄÃ£ cache: {len(cached_urls)} ({len(cached_urls)*100//total if total > 0 else 0}%)")
    print(f"   - ChÆ°a cache: {len(uncached_urls)} ({len(uncached_urls)*100//total if total > 0 else 0}%)")
    print(f"{'='*60}\n")
    
    # Hiá»ƒn thá»‹ URLs Ä‘Ã£ cache
    if cached_urls and args.show_cached:
        print(f"âœ… URLs Ä‘Ã£ cache ({min(args.limit, len(cached_urls))} Ä‘áº§u tiÃªn):")
        for item in cached_urls[:args.limit]:
            url = item.get("url") if isinstance(item, dict) else item
            print(f"   {url}")
        if len(cached_urls) > args.limit:
            print(f"   ... vÃ  {len(cached_urls) - args.limit} URLs khÃ¡c")
        print()
    
    # Hiá»ƒn thá»‹ URLs chÆ°a cache
    if uncached_urls and args.show_uncached:
        print(f"âš ï¸  URLs chÆ°a cache ({min(args.limit, len(uncached_urls))} Ä‘áº§u tiÃªn):")
        for item in uncached_urls[:args.limit]:
            url = item.get("url") if isinstance(item, dict) else item
            print(f"   {url}")
        if len(uncached_urls) > args.limit:
            print(f"   ... vÃ  {len(uncached_urls) - args.limit} URLs khÃ¡c")
        print()
    
    # LÆ°u danh sÃ¡ch URLs Ä‘Ã£ cache
    if cached_urls:
        with open(args.output_cached, "w", encoding="utf-8") as f:
            json.dump({
                "total_urls": len(cached_urls),
                "description": f"URLs Ä‘Ã£ cache tá»« {args.json_file}",
                "urls": cached_urls
            }, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ÄÃ£ lÆ°u {len(cached_urls)} URLs Ä‘Ã£ cache vÃ o: {args.output_cached}")
    
    # LÆ°u danh sÃ¡ch URLs chÆ°a cache
    if uncached_urls:
        with open(args.output_uncached, "w", encoding="utf-8") as f:
            json.dump({
                "total_urls": len(uncached_urls),
                "description": f"URLs chÆ°a cache tá»« {args.json_file}",
                "urls": uncached_urls
            }, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ ÄÃ£ lÆ°u {len(uncached_urls)} URLs chÆ°a cache vÃ o: {args.output_uncached}")
    
    # Gá»£i Ã½ crawl URLs chÆ°a cache
    if uncached_urls:
        print(f"\nğŸ’¡ Äá»ƒ crawl {len(uncached_urls)} URLs chÆ°a cache:")
        print(f"   python crawl_from_json.py {args.output_uncached} --follow-depth 2 --concurrency 4 --delay 0.5")

if __name__ == "__main__":
    main()


