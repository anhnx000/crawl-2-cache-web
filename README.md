
# kiagds_local_cache

Proxy + cache Ä‘á»ƒ duyá»‡t vÃ  xem **kiagds.ru** trÃªn **localhost:5002** (online láº§n Ä‘áº§u Ä‘á»ƒ cache, sau Ä‘Ã³ cÃ³ thá»ƒ offline). 
KÃ¨m theo script **warm_ajax.py** Ä‘á»ƒ pre-warm cÃ¡c endpoint Ajax (`?mode=...&docId=...`) tá»« cÃ¢y menu báº¡n cung cáº¥p.

> âš ï¸ Vui lÃ²ng chá»‰ dÃ¹ng cho má»¥c Ä‘Ã­ch cÃ¡ nhÃ¢n/há»£p phÃ¡p vÃ  tÃ´n trá»ng robots.txt/ToS cá»§a website Ä‘Ã­ch.

## YÃªu cáº§u há»‡ thá»‘ng
- **Ubuntu Linux** (khÃ´ng há»— trá»£ Windows/macOS)
- **Conda** (Anaconda hoáº·c Miniconda)
- **uv** (package manager)

## Cáº¥u trÃºc
```
kiagds_local_cache/
â”œâ”€ app.py                   # Reverse-proxy + cache (cá»•ng 5002)
â”œâ”€ warm_ajax.py             # Pre-warm cache cho cÃ¡c endpoint Ajax (?docId=...)
â”œâ”€ async_crawl.py           # (tuá»³ chá»n) Crawler async httpx + BeautifulSoup
â”œâ”€ auto_crawl_proxy.py      # Auto crawler qua proxy, tá»± Ä‘á»™ng extract vÃ  crawl links
â”œâ”€ crawl_from_json.py        # Helper script Ä‘á»ƒ crawl tá»« file JSON
â”œâ”€ capture_with_playwright.py # (tuá»³ chá»n) Ghi má»i GET khi báº¡n duyá»‡t báº±ng Playwright
â”œâ”€ pyproject.toml           # Cáº¥u hÃ¬nh dependencies cho uv
â”œâ”€ requirements.txt         # (giá»¯ láº¡i cho tÆ°Æ¡ng thÃ­ch)
â”œâ”€ setup.sh                 # Script tá»± Ä‘á»™ng thiáº¿t láº­p mÃ´i trÆ°á»ng
â”œâ”€ Dockerfile               # (tuá»³ chá»n) cháº¡y proxy qua Docker
â”œâ”€ .gitignore
â””â”€ README.md
```
ThÆ° má»¥c **cache/** sáº½ Ä‘Æ°á»£c táº¡o tá»± Ä‘á»™ng á»Ÿ láº§n cháº¡y Ä‘áº§u.

## CÃ i Ä‘áº·t (Ubuntu Linux vá»›i Conda vÃ  uv)

### 1. CÃ i Ä‘áº·t Conda (náº¿u chÆ°a cÃ³)
```bash
# Táº£i vÃ  cÃ i Ä‘áº·t Miniconda
wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh
# Sau khi cÃ i, khá»Ÿi Ä‘á»™ng láº¡i terminal hoáº·c cháº¡y:
source ~/.bashrc
```

### 2. CÃ i Ä‘áº·t uv
```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
source ~/.bashrc
```

### 3. Thiáº¿t láº­p mÃ´i trÆ°á»ng Conda vÃ  cÃ i Ä‘áº·t dependencies

**CÃ¡ch 1: Sá»­ dá»¥ng script tá»± Ä‘á»™ng (khuyáº¿n nghá»‹)**
```bash
chmod +x setup.sh
./setup.sh
```

**CÃ¡ch 2: Thiáº¿t láº­p thá»§ cÃ´ng**
```bash
# Táº¡o hoáº·c kÃ­ch hoáº¡t mÃ´i trÆ°á»ng conda tÃªn "crawl"
conda create -n crawl python=3.11 -y
conda activate crawl

# CÃ i Ä‘áº·t dependencies báº±ng uv
uv pip install -r requirements.txt
# Hoáº·c sá»­ dá»¥ng pyproject.toml:
uv pip install -e .
```

### 4. KÃ­ch hoáº¡t mÃ´i trÆ°á»ng Ä‘á»ƒ sá»­ dá»¥ng
```bash
conda activate crawl
```

## Cháº¡y proxy (láº§n Ä‘áº§u online Ä‘á»ƒ cache)
```bash
# Äáº£m báº£o Ä‘Ã£ kÃ­ch hoáº¡t mÃ´i trÆ°á»ng conda
conda activate crawl

export LIVE_FALLBACK=true
python app.py
# Má»Ÿ trÃ¬nh duyá»‡t: http://localhost:5002/
# VÃ­ dá»¥ URL gá»‘c (giá»¯ nguyÃªn tham sá»‘):
# http://localhost:5002/?mode=ETM&marke=KM&year=2026&model=9193&mkb=447__29696&docId=435525
```

## Cháº¡y OFFLINE
Sau khi Ä‘Ã£ cache Ä‘á»§:
```bash
# Äáº£m báº£o Ä‘Ã£ kÃ­ch hoáº¡t mÃ´i trÆ°á»ng conda
conda activate crawl

export LIVE_FALLBACK=false
python app.py
```

## Pre-warm Ajax theo cÃ¢y menu
1) LÆ°u HTML/Ä‘oáº¡n menu cÃ³ chá»©a `docId` vÃ o **menu.txt** (vÃ­ dá»¥ báº¡n Ä‘Ã£ gá»­i).
2) Cháº¡y:
```bash
# Äáº£m báº£o Ä‘Ã£ kÃ­ch hoáº¡t mÃ´i trÆ°á»ng conda
conda activate crawl

# proxy Ä‘ang cháº¡y á»Ÿ terminal khÃ¡c (LIVE_FALLBACK=true)
python warm_ajax.py --html menu.txt   --marke KM --year 2026 --model 9193 --mkb 447__29696   --start-page 1 --end-page 1
```
TÄƒng `--end-page` náº¿u endpoint cÃ³ phÃ¢n trang `&page=`.

## Auto Crawler - Tá»± Ä‘á»™ng crawl vÃ  cache toÃ n bá»™ trang web

Script `auto_crawl_proxy.py` tá»± Ä‘á»™ng crawl qua proxy, extract cÃ¡c links tá»« HTML vÃ  crawl tiáº¿p Ä‘á»ƒ táº¡o báº£n mirror Ä‘áº§y Ä‘á»§.

### YÃªu cáº§u:
- Proxy pháº£i Ä‘ang cháº¡y vá»›i `LIVE_FALLBACK=true` (terminal khÃ¡c)
- Script sáº½ tá»± Ä‘á»™ng kiá»ƒm tra proxy trÆ°á»›c khi báº¯t Ä‘áº§u crawl

### CÃ¡ch sá»­ dá»¥ng:

**1. Crawl tá»« trang chá»§ vÃ  tá»± Ä‘á»™ng follow táº¥t cáº£ links:**
```bash
conda activate crawl
python auto_crawl_proxy.py --follow-depth 3 --concurrency 4 --delay 0.5
```

**2. Crawl tá»« seed URL cá»¥ thá»ƒ vá»›i pagination:**
```bash
conda activate crawl
python auto_crawl_proxy.py \
  --seed "https://kiagds.ru/?mode=ETM&marke=KM&year=2026&model=9193&mkb=447__29696&docId=435525&page=" \
  --start-page 1 \
  --end-page 50 \
  --follow-depth 2 \
  --concurrency 4 \
  --delay 0.5
```

**3. Crawl vá»›i nhiá»u URL bá»• sung:**
```bash
conda activate crawl
python auto_crawl_proxy.py \
  --extra-urls "https://kiagds.ru/page1" "https://kiagds.ru/page2" \
  --follow-depth 5 \
  --concurrency 6 \
  --delay 0.3 \
  --verbose
```

**4. Crawl tá»« file JSON (khuyáº¿n nghá»‹ cho sá»‘ lÆ°á»£ng lá»›n):**
```bash
conda activate crawl

# CÃ¡ch 1: Sá»­ dá»¥ng auto_crawl_proxy.py trá»±c tiáº¿p (Ä‘Æ¡n giáº£n)
# Crawl táº¥t cáº£ URLs tá»« file JSON
python auto_crawl_proxy.py \
  --json-file full_urls_to_crawl.json \
  --follow-depth 2 \
  --concurrency 4 \
  --delay 0.5

# Crawl má»™t pháº§n URLs (tá»« index 0 Ä‘áº¿n 1000)
python auto_crawl_proxy.py \
  --json-file full_urls_to_crawl.json \
  --json-start-index 0 \
  --json-end-index 1000 \
  --follow-depth 2 \
  --concurrency 4 \
  --delay 0.5 \
  --verbose

# CÃ¡ch 2: Sá»­ dá»¥ng crawl_from_json.py (chia thÃ nh batches)
# Xem URLs sáº½ crawl (dry-run)
python crawl_from_json.py full_urls_to_crawl.json --dry-run

# Crawl táº¥t cáº£ URLs (chia thÃ nh batches)
python crawl_from_json.py full_urls_to_crawl.json \
  --batch-size 100 \
  --follow-depth 2 \
  --concurrency 4 \
  --delay 0.5

# Crawl má»™t pháº§n URLs (tá»« index 0 Ä‘áº¿n 1000)
python crawl_from_json.py full_urls_to_crawl.json \
  --start-index 0 \
  --end-index 1000 \
  --batch-size 50 \
  --verbose
```

### CÃ¡c tham sá»‘:

#### `auto_crawl_proxy.py`:
- `--seed`: URL seed cÃ³ pháº§n `&page=` Ä‘á»ƒ crawl nhiá»u trang
- `--start-page`, `--end-page`: Pháº¡m vi trang Ä‘á»ƒ crawl (náº¿u seed cÃ³ pagination)
- `--extra-urls`: Danh sÃ¡ch URL bá»• sung Ä‘á»ƒ crawl
- `--json-file`: ÄÆ°á»ng dáº«n file JSON chá»©a danh sÃ¡ch URLs Ä‘á»ƒ crawl (khuyáº¿n nghá»‹ cho sá»‘ lÆ°á»£ng lá»›n)
- `--json-start-index`: Chá»‰ sá»‘ báº¯t Ä‘áº§u khi Ä‘á»c tá»« JSON (máº·c Ä‘á»‹nh: 0)
- `--json-end-index`: Chá»‰ sá»‘ káº¿t thÃºc khi Ä‘á»c tá»« JSON (máº·c Ä‘á»‹nh: táº¥t cáº£)
- `--follow-depth`: Äá»™ sÃ¢u crawl links (0 = chá»‰ seeds, >0 = tá»± Ä‘á»™ng crawl links trong HTML, máº·c Ä‘á»‹nh: 3)
- `--concurrency`: Sá»‘ lÆ°á»£ng request Ä‘á»“ng thá»i (máº·c Ä‘á»‹nh: 4)
- `--delay`: GiÃ£n cÃ¡ch giá»¯a cÃ¡c request - giÃ¢y (máº·c Ä‘á»‹nh: 0.5)
- `--verbose`: Hiá»ƒn thá»‹ chi tiáº¿t (cached URLs vÃ  links found)
- `--auto-pagination`: Tá»± Ä‘á»™ng phÃ¡t hiá»‡n vÃ  crawl pagination (máº·c Ä‘á»‹nh: True)
- `--proxy-base`: URL proxy base náº¿u khÃ¡c máº·c Ä‘á»‹nh (máº·c Ä‘á»‹nh: http://localhost:5002)
- `--max-retries`: Sá»‘ láº§n retry khi gáº·p lá»—i network/timeout (máº·c Ä‘á»‹nh: 10)

#### `crawl_from_json.py`:
- `json_file`: ÄÆ°á»ng dáº«n file JSON chá»©a danh sÃ¡ch URLs (required)
- `--batch-size`: Sá»‘ lÆ°á»£ng URLs crawl má»—i láº§n (máº·c Ä‘á»‹nh: 100)
- `--start-index`: Chá»‰ sá»‘ báº¯t Ä‘áº§u (máº·c Ä‘á»‹nh: 0)
- `--end-index`: Chá»‰ sá»‘ káº¿t thÃºc (máº·c Ä‘á»‹nh: táº¥t cáº£)
- `--follow-depth`: Äá»™ sÃ¢u crawl links (máº·c Ä‘á»‹nh: 2)
- `--concurrency`: Sá»‘ lÆ°á»£ng request Ä‘á»“ng thá»i (máº·c Ä‘á»‹nh: 4)
- `--delay`: GiÃ£n cÃ¡ch giá»¯a cÃ¡c request - giÃ¢y (máº·c Ä‘á»‹nh: 0.5)
- `--verbose`: Hiá»ƒn thá»‹ chi tiáº¿t
- `--dry-run`: Chá»‰ hiá»ƒn thá»‹ URLs sáº½ crawl, khÃ´ng thá»±c sá»± crawl

### TÃ­nh nÄƒng chÃ­nh:

#### ğŸ” Tá»± Ä‘á»™ng phÃ¡t hiá»‡n Pagination:
- **"Page X of Y"**: Tá»± Ä‘á»™ng phÃ¡t hiá»‡n pattern "Page 1 of 17" vÃ  crawl táº¥t cáº£ 17 trang
- **Pagination links**: PhÃ¡t hiá»‡n cÃ¡c sá»‘ trang trong onclick handlers (vÃ­ dá»¥: `ajaxHref('?page=1')`) vÃ  crawl táº¥t cáº£
- Tá»± Ä‘á»™ng giá»¯ nguyÃªn cÃ¡c query parameters khÃ¡c khi táº¡o URLs pagination

#### ğŸ”— Extract Links thÃ´ng minh:
- Extract tá»« HTML tags: `<a>`, `<link>`, `<script>`, `<img>`, `<iframe>`, `<form>`
- Extract tá»« JavaScript: TÃ¬m URLs trong code JavaScript
- Extract tá»« onclick handlers: PhÃ¡t hiá»‡n `ajaxHref()`, `location.href`, etc.
- Extract tá»« data-* attributes: TÃ¬m URLs trong cÃ¡c thuá»™c tÃ­nh data-*

#### âœ¨ Tá»± Ä‘á»™ng lÃ m sáº¡ch URL:
- Loáº¡i bá» `&page=` rá»—ng á»Ÿ cuá»‘i URL
- Chuáº©n hÃ³a query parameters
- Loáº¡i bá» fragment (#)

#### ğŸ›¡ï¸ Error Handling & Retry:
- Tá»± Ä‘á»™ng kiá»ƒm tra proxy trÆ°á»›c khi crawl
- Hiá»ƒn thá»‹ thÃ´ng tin proxy (cached responses, live_fallback)
- Dá»«ng ngay náº¿u proxy khÃ´ng cháº¡y vá»›i thÃ´ng bÃ¡o rÃµ rÃ ng
- Bá» qua URLs Ä‘Ã£ cache Ä‘á»ƒ trÃ¡nh crawl láº¡i
- **Auto retry vá»›i exponential backoff**: Tá»± Ä‘á»™ng retry tá»‘i Ä‘a 10 láº§n (cÃ³ thá»ƒ tÃ¹y chá»‰nh) khi gáº·p lá»—i network/timeout
- Exponential backoff: 1s, 2s, 4s, 8s, max 10s giá»¯a cÃ¡c láº§n retry
- KhÃ´ng retry náº¿u proxy khÃ´ng cháº¡y (fail fast)
- Log chi tiáº¿t quÃ¡ trÃ¬nh retry Ä‘á»ƒ debug
- Xá»­ lÃ½ lá»—i tá»‘t, khÃ´ng crash khi má»™t sá»‘ trang cÃ³ váº¥n Ä‘á»

#### ğŸ“Š Thá»‘ng kÃª vÃ  Progress:
- Hiá»ƒn thá»‹ sá»‘ URLs Ä‘Ã£ cache sáºµn
- Hiá»ƒn thá»‹ sá»‘ URLs má»›i crawl
- Hiá»ƒn thá»‹ sá»‘ lá»—i
- Hiá»ƒn thá»‹ tá»•ng sá»‘ URLs Ä‘Ã£ xá»­ lÃ½
- ThÃ´ng bÃ¡o khi phÃ¡t hiá»‡n pagination

### VÃ­ dá»¥ Output:
```
ğŸ” Kiá»ƒm tra proxy http://localhost:5002...
âœ… Proxy Ä‘ang cháº¡y
   - Cached responses: 97
   - Live fallback: true

ğŸ“‹ Seed URLs: 1
   - https://kiagds.ru/?mode=ETM&marke=KM&year=2026&model=9193&mkb=447__29696&docId=435662

âœ… [200] https://kiagds.ru/?mode=ETM&marke=KM&year=2026&model=9193&mkb=447__29696&docId=435662 (depth=0)
  ğŸ“„ PhÃ¡t hiá»‡n pagination: page_of, max_page=17
  ğŸ“„ ÄÃ£ thÃªm 17 trang pagination vÃ o queue
  ğŸ”— Found 124 new links (total: 125)

âœ… [200] https://kiagds.ru/?mode=ETM&marke=KM&year=2026&model=9193&mkb=447__29696&docId=435662&page=1 (depth=0)
...

============================================================
âœ… HoÃ n thÃ nh crawl!
   - ÄÃ£ cache sáºµn: 0 URLs
   - Má»›i crawl: 150 URLs
   - Lá»—i: 0 URLs
   - Tá»•ng URLs Ä‘Ã£ xá»­ lÃ½: 150 URLs
============================================================
```

## (Tuá»³ chá»n) Crawler async (crawl trá»±c tiáº¿p tá»« origin)
```bash
# Äáº£m báº£o Ä‘Ã£ kÃ­ch hoáº¡t mÃ´i trÆ°á»ng conda
conda activate crawl

python async_crawl.py   --seed "https://kiagds.ru/?mode=ETM&marke=KM&year=2026&model=9193&mkb=447__29696&docId=435525&page="   --start-page 1 --end-page 50 --follow-depth 0
```

## (Tuá»³ chá»n) Capture báº±ng Playwright
> Playwright **khÃ´ng** náº±m trong requirements máº·c Ä‘á»‹nh. Náº¿u muá»‘n dÃ¹ng:
```bash
# Äáº£m báº£o Ä‘Ã£ kÃ­ch hoáº¡t mÃ´i trÆ°á»ng conda
conda activate crawl

# CÃ i Ä‘áº·t playwright báº±ng uv
uv pip install playwright
playwright install
python capture_with_playwright.py "https://kiagds.ru/?mode=ETM&marke=KM&year=2026&model=9193&mkb=447__29696&docId=435525"
```

## Docker (tuá»³ chá»n, cho app.py)
> Dockerfile Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t Ä‘á»ƒ sá»­ dá»¥ng Ubuntu vÃ  conda/uv:
```bash
docker build -t kiagds-cache .
docker run --rm -p 5002:5002 -e LIVE_FALLBACK=true kiagds-cache
```

## Biáº¿n mÃ´i trÆ°á»ng
- `LIVE_FALLBACK=true|false` â€” Cho phÃ©p proxy gá»i origin khi cache miss (true) hay chá»‰ phá»¥c vá»¥ offline (false). Máº·c Ä‘á»‹nh **true**.

## Format File JSON cho crawl_from_json.py

File JSON cÃ³ thá»ƒ cÃ³ cÃ¡c format sau:

**Format 1: Object vá»›i array urls**
```json
{
  "total_urls": 12484,
  "urls": [
    {"url": "https://kiagds.ru/?mode=ETM&..."},
    {"url": "https://kiagds.ru/?mode=ETM&..."}
  ]
}
```

**Format 2: Array trá»±c tiáº¿p**
```json
[
  {"url": "https://kiagds.ru/?mode=ETM&..."},
  {"url": "https://kiagds.ru/?mode=ETM&..."}
]
```

**Format 3: Array URLs Ä‘Æ¡n giáº£n**
```json
[
  "https://kiagds.ru/?mode=ETM&...",
  "https://kiagds.ru/?mode=ETM&..."
]
```

## Troubleshooting

### Lá»—i "Connection refused" hoáº·c "All connection attempts failed"
**NguyÃªn nhÃ¢n**: Proxy khÃ´ng cháº¡y hoáº·c khÃ´ng thá»ƒ káº¿t ná»‘i tá»›i proxy.

**Giáº£i phÃ¡p**:
1. Kiá»ƒm tra proxy cÃ³ Ä‘ang cháº¡y:
   ```bash
   curl http://localhost:5002/_cache_stats
   ```
2. Náº¿u khÃ´ng cháº¡y, khá»Ÿi Ä‘á»™ng proxy:
   ```bash
   conda activate crawl
   export LIVE_FALLBACK=true
   conda run -n crawl python app.py
   ```

### Lá»—i "Proxy khÃ´ng thá»ƒ káº¿t ná»‘i"
**NguyÃªn nhÃ¢n**: Proxy chÆ°a Ä‘Æ°á»£c khá»Ÿi Ä‘á»™ng hoáº·c Ä‘ang cháº¡y á»Ÿ cá»•ng khÃ¡c.

**Giáº£i phÃ¡p**:
- Äáº£m báº£o proxy Ä‘ang cháº¡y á»Ÿ cá»•ng 5002
- Náº¿u proxy cháº¡y á»Ÿ cá»•ng khÃ¡c, sá»­ dá»¥ng `--proxy-base http://localhost:<PORT>`

### URLs cÃ³ `&page=` rá»—ng bá»‹ lá»—i
**NguyÃªn nhÃ¢n**: URL cÃ³ parameter `page` rá»—ng (vÃ­ dá»¥: `&page=`).

**Giáº£i phÃ¡p**: Script tá»± Ä‘á»™ng xá»­ lÃ½ vÃ  loáº¡i bá» parameter rá»—ng. Náº¿u váº«n lá»—i, kiá»ƒm tra URL format.

### Crawl cháº­m hoáº·c bá»‹ timeout
**Giáº£i phÃ¡p**:
- TÄƒng `--delay` Ä‘á»ƒ giáº£m táº£i (vÃ­ dá»¥: `--delay 1.0`)
- Giáº£m `--concurrency` (vÃ­ dá»¥: `--concurrency 2`)
- Kiá»ƒm tra káº¿t ná»‘i máº¡ng vÃ  tá»‘c Ä‘á»™ cá»§a origin server

### Pagination khÃ´ng Ä‘Æ°á»£c phÃ¡t hiá»‡n
**NguyÃªn nhÃ¢n**: Pagination cÃ³ format khÃ¡c hoáº·c khÃ´ng cÃ³ trong HTML.

**Giáº£i phÃ¡p**:
- Kiá»ƒm tra HTML cá»§a trang Ä‘á»ƒ xem format pagination
- Sá»­ dá»¥ng `--verbose` Ä‘á»ƒ xem chi tiáº¿t
- Náº¿u cáº§n, sá»­ dá»¥ng `--seed` vá»›i `--start-page` vÃ  `--end-page` Ä‘á»ƒ crawl thá»§ cÃ´ng

## Best Practices

1. **LuÃ´n cháº¡y proxy vá»›i `LIVE_FALLBACK=true` khi crawl** Ä‘á»ƒ Ä‘áº£m báº£o cache Ä‘Æ°á»£c táº¡o.

2. **Sá»­ dá»¥ng delay há»£p lÃ½** (0.5-1.0 giÃ¢y) Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i server.

3. **Chia nhá» khi crawl sá»‘ lÆ°á»£ng lá»›n URLs**:
   - Sá»­ dá»¥ng `crawl_from_json.py` vá»›i `--batch-size`
   - Crawl tá»«ng pháº§n vá»›i `--start-index` vÃ  `--end-index`

4. **Kiá»ƒm tra cache trÆ°á»›c khi crawl láº¡i**:
   - Script tá»± Ä‘á»™ng bá» qua URLs Ä‘Ã£ cache
   - Kiá»ƒm tra sá»‘ lÆ°á»£ng cached: `curl http://localhost:5002/_cache_stats`

5. **Sá»­ dá»¥ng `--verbose` khi debug** Ä‘á»ƒ xem chi tiáº¿t quÃ¡ trÃ¬nh crawl.

6. **LÆ°u Ã½ vá» pagination**:
   - Auto pagination detection hoáº¡t Ä‘á»™ng tá»‘t vá»›i format chuáº©n
   - Náº¿u khÃ´ng phÃ¡t hiá»‡n Ä‘Æ°á»£c, sá»­ dá»¥ng `--seed` vá»›i `--start-page` vÃ  `--end-page`

## LÆ°u Ã½
- Chá»‰ **GET** Ä‘Æ°á»£c cache. Náº¿u trang cÃ³ POST, Ä‘Äƒng nháº­p, captchaâ€¦ thÃ¬ offline khÃ´ng tÃ¡i táº¡o Ä‘áº§y Ä‘á»§ cÃ¡c pháº§n Ä‘Ã³.
- Proxy cÃ³ **rewrite** URL tuyá»‡t Ä‘á»‘i `https://kiagds.ru` vÃ  cáº£ dáº¡ng `//kiagds.ru` vá» `http://localhost:5002` trong HTML/JS/CSS.
- Throttle/giÃ£n cÃ¡ch khi warm Ä‘á»ƒ lá»‹ch sá»± vá»›i website Ä‘Ã­ch.
- **Auto pagination** chá»‰ hoáº¡t Ä‘á»™ng khi phÃ¡t hiá»‡n Ä‘Æ°á»£c pattern "Page X of Y" hoáº·c pagination links trong HTML.
- URLs cÃ³ `&page=` rá»—ng sáº½ Ä‘Æ°á»£c tá»± Ä‘á»™ng lÃ m sáº¡ch.
# crawl-2-cache-web
